[{"content":" 原文标题：Speed Up Your Python Program With Concurrency\n原文作者：Jim Anderson\n原文链接：https://realpython.com/python-concurrency/\n如果你曾听过很多关于 asyncio 即将加入 Python 标准库的讨论(译者注: asyncio于Python3.4时被引入标准库，于Python3.6基本稳定)，但很是好奇它和其他并发方法相比有何异同；或者想知道并发到底是什么，以及它是如何为你的程序加速的；那你就来对地方啦~\n在本文中，你会了解到以下知识：\n什么是 并发 什么是 并行 Python 中一些 并发方法 的比较，包括线程、进程、异步I/O 何时 需要在程序中 使用并发，用哪个模块 本文需要读者对 Python 有一些基本的理解。文中代码示例最低可以使用 Python 3.6 来运行。读者可以从 RealPython Github repo 中下载这些代码。\n什么是并发 字典中对于并行的定义是，同时发生。在 Python 中，对于同时发生的事情有很多个称谓(线程、任务、进程)，但是从本质上看，它们都指代的是一系列按顺序运行的指令。\n我喜欢把它们比作是不同的“想法”，每一个想法都可以在某个时间点停下，然后处理它们的 CPU 或大脑改为“思考”另一个想法。每个想法的状态都会被保存下来，以便于它们再被“想起”时可以从上次中断的地方开始。\n你可能想知道为什么 Python 对于同一个概念会有不同的称谓，但事实上，线程、任务、进程只有从本质上看时才是相同的。一旦你开始深挖细节，会发现它们其实代表的并不是同一种事物。跟随本文的脚步，你会发现它们之间更多的不同。\n现在，我们开始讨论定义中“同时”的含义。当我们深入了解其细节时必须要严谨一些，因为只有 多进程 才能真正的同时“思考”多个“想法”。线程(threading 库)和异步(asyncio 库)底层还是在同一个处理器上运行，因此同一时刻还是只能运行一个。它们只是运用一些巧妙的方式提升了依次执行的效率，进而加速整个处理过程。所以尽管它们并没有真正的让机器同时“思考”不同的“想法”，但我们也仍会称它们为并发。\n多线程和异步最大的区别在于它们处理线程和任务顺序执行时使用的方式。对于多线程来说，操作系统清楚地了解每一个线程，也可以随时中断一个线程并启动另一个。正是由于操作系统具有这种可以随时剥夺一个线程执行权然后直接给另一个的能力，所以这种方式被称为抢占式多任务处理。\n抢占式多任务处理非常方便，因为线程中的代码不需要为CPU控制权转换做任何事情。但是，代码也可能会因为“随时”而变得非常复杂。因为线程切换可能发生在任意一个独立的 Python 语句中间，即使是 x = x + 1 这种微不足道的语句。\n而对于异步I/O来讲，它使用的是协作式多任务处理。多个任务之间若要实现协作，就必须宣告它们自己何时退出占用。这也意味着，相比于顺序执行，多任务中的代码必须稍微调整一下才能实现这一点。\n提前做这些额外工作的好处就在于，开发人员总能知道任务会在哪里退出占用。只要一段 Python 程序没有被标记，那么它就绝不会在中间退出占用。在后面的章节中，我们会继续介绍异步是如何简化部分设计的。\n什么是并行 看到这里，我们已经介绍了在单核处理器上发生的并发。那么，你那酷酷的新笔记本电脑所拥有的多核处理器会有怎样的船新体验呢？你又该如何使用它们呢？多进程告诉你答案。\nPython 可以通过 multiprocessing 库创建新的进程。虽然从技术角度上看，进程被定义为一系列像内存、文件句柄这样的资源集合，而这里的进程可以被认为是一个完全不同的程序。也就是说我们可以这么想，每一个进程在它自己的 Python 解释器中运行。\n因为是它们是不同的进程，所以在多进程程序中的每一个“思路”都可以在不同的处理器核中运行。可以在不同核中运行也就意味着可以真正做到让它们在相同时刻运行，如同天上降魔主，真是人间太岁神，这个多进程__(发送评论查询译者精神状况ヾ(≧▽≦*)o)。虽然这样做会增加一些复杂性，但是 Python 在大多数情况下都能很好的解决这些问题。\n想必你对并发和并行已经有了些许概念，那现在我们来复习一下它们的区别，后面我们再看看它们究竟为什么这么有用：\n并发类型 执行权的决定因素 处理器数量 抢占式多任务处理(threading) 由操作系统决定何时切换 1 协作式多任务处理(asyncio) 由任务本身决定和放弃控制权 1 多进程(multiprocessing) 进程们同时在多个核上运行 很多 这些并发都很有用，接下来我们来看看它们各自会在什么情况下能提升速度。\n并发什么时候有用 有两类问题在面对并发时会有非常大的不同，通常它们被称为计算密集型和 I/O 密集型问题。\nI/O 密集型问题造成程序速度慢的原因在于它必须频繁的等待外部资源的输入或输出。当你的程序和一些比 CPU 慢很多的事物一起运作时，这种问题就会频繁出现。\n比 CPU 慢的事物数不胜数，但好在它们大部分都不会和你的程序有交互。而在剩下的那些当中，和程序交互最频繁的就是文件系统和网络连接了。\n下图将这种交互描绘的非常形象： 图中蓝色的方块表示程序执行消耗的时间，红色的方块表示等待 I/O 操作直到完成所消耗的时间。方框大小的比例并不指代真实的耗时比例，因为网络请求的耗时会比 CPU 指令多几个数量级，因此你的程序可能大部分时间都花在等待上了。这也是浏览器这种程序大部分时间在做的事情hhh\n在另一个极端，有些程序并没有多少网络请求或者文件访问，但需要做大量的运算。由于这些程序限制速度的瓶颈在于 CPU 的运算效率而不是网络连接或文件系统，所以这类程序就被称为计算密集型程序。\n下图是计算密集型程序相应的图： 在下一节的示例中，你将看到不同种类的并发在计算密集型和I/O密集型程序中优劣。在程序中使用并发会增加额外代码和复杂度，所以你需要权衡潜在的速度提升是否值得花费这些额外的代价。等你看到文章的结尾，也许就有足够的信息来做权衡了。\n这里是一些解释概念的的简要概述：\nI/O 密集型程序 计算密集型程序 程序的大部分时间花费在和相对较慢的设备做交互上，比如网络连接，硬盘或者打印机 程序的大部分时间花费在做 CPU 运算上 提升速度需要尽可能将不同设备的等待时间重叠在一起 提升速度需要让程序在相同时间内做更多的运算 接下来我们先介绍 I/O 密集型程序再看计算密集型程序。\n如何提升I/O密集型程序的速度 我们来聚焦一下 I/O 密集型程序面临的一个常见的需求：从网络上下载。我们的例子实现了从一些站点中下载 web 页面的功能，当然，任何网络资源都可以，下载页面只是因为它更容易展示和启动。\n同步版本 对于这个任务，我们先从非并发版本开始。需要注意的是，这个程序依赖 requests 库，即运行之前需要执行 pip install requests。这个版本一点并发都没用：\nimport requests import time def download_site(url, session): with session.get(url) as response: print(f\u0026#34;Read {len(response.content)} from {url}\u0026#34;) def download_all_sites(sites): with requests.Session() as session: for url in sites: download_site(url, session) if __name__ == \u0026#34;__main__\u0026#34;: sites = [ \u0026#34;https://www.jython.org\u0026#34;, \u0026#34;http://olympus.realpython.org/dice\u0026#34;, ] * 80 start_time = time.time() download_all_sites(sites) duration = time.time() - start_time print(f\u0026#34;Downloaded {len(sites)} in {duration} seconds\u0026#34;) 如你所见，这程序真的相当短小了。download_site() 函数仅仅是下载了 URL 里面的内容，同时打印了内容的大小。值得注意的一点是，我们使用了 requests 库中的 Session 对象。\n直接简单的使用 requests 中的 get() 函数当然也可以，但是创建一个 Session 对象能让 requests 做一些有趣的网络技巧，还能提升速度。\ndownload_all_sites() 函数先创建了 Session 对象，然后遍历 sites 列表再依次下载。最后它打印出整个过程的耗时，这样在后面的例子中，我们就可以欣慰的看到使用并发能带来多大的帮助。\n这个程序的流程图和上一小节的 I/O 密集型程序的图非常相似。\n注意：影响网速的因素有很多，甚至每秒都在变化。测试的时候我曾遇见过因为网速的原因导致运行时间多了一倍。\n为什么同步版本如此流行 这个版本的伟大之处就在于，它非常简单。开发人员可以更容易的编写和调试，考虑问题时也能更直接。只有一条思路贯穿始终，我们就可以预测下一步是什么以及它会有怎样的表现。\n同步版本的缺陷 最大的问题在于，相对于后面提供的其他解决方案，同步版本慢得可怕。下面是在我的机器上运行的输出；\n$ ./io_non_concurrent.py [most output skipped] Downloaded 160 in 14.289619207382202 seconds 注意：你的结果可能和这个出入很大，运行脚本时耗时大概在 14.2 - 21.9 秒左右。本文中我使用了三次实验中最快的一个数值，但和其他方式相比，差距还是十分明显。\n其实程序运行的相对较慢不是什么大事，如果你的程序使用同步版本只用了2秒，而且运行的次数也非常稀少，那可能加并发也没太大必要。也就没必要再往下看了\n那如果程序运行的非常频繁呢？如果运行一次就要好几个小时呢？来来来，移步到并发这边，我们使用 threading 重新一下刚刚的程序。\nthreading 多线程版本 你可能会想，写一个多线程程序是不是很难啊？表慌，你马上就会惊诧对于简单情况额外的修改竟然就这么点儿。下面就是使用 threading 库改造后的版本了：\nimport concurrent.futures import requests import threading import time thread_local = threading.local() def get_session(): if not hasattr(thread_local, \u0026#34;session\u0026#34;): thread_local.session = requests.Session() return thread_local.session def download_site(url): session = get_session() with session.get(url) as response: print(f\u0026#34;Read {len(response.content)} from {url}\u0026#34;) def download_all_sites(sites): with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: executor.map(download_site, sites) if __name__ == \u0026#34;__main__\u0026#34;: sites = [ \u0026#34;https://www.jython.org\u0026#34;, \u0026#34;http://olympus.realpython.org/dice\u0026#34;, ] * 80 start_time = time.time() download_all_sites(sites) duration = time.time() - start_time print(f\u0026#34;Downloaded {len(sites)} in {duration} seconds\u0026#34;) 引入 threading 时，程序的整体结构并没有发生改变，只需要做一些小小的调整。download_all_sites() 函数从之前的遍历每个站点调用方法变为了一种复杂些的结构。\n在这个版本中，我们创建了一个 ThreadPoolExecutor，看起来好复杂的样子。我们可以把它的名字拆开来看：ThreadPoolExecutor = Thread + Pool + Executor。\n之前我们提到过 Thread，它可以被看作一个思路。Pool 就开始变得有趣了，它可以创建一池子的线程，其中每一个都能并发执行。最后，Executor 可以控制池子中的那些线程什么时候以及怎样运行。它将在池子中执行 request 请求。\n好在，标准库将 ThreadPoolExecutor 实现为上下文管理器，我们可以使用 with 语法来管理创建和释放池中的线程。\n一旦拥有了 ThreadPoolExecutor，我们就可以使用它方便的 .map() 方法。这个方法会让 sites 列表中的每一个元素运行传入的函数。最妙的一点在于，它可以控制线程池让其中线程自动并发的执行这些函数。\n使用其他语言(包括 Python 2)的同学可能想知道，像类似 Thread.start()、Thread.join()、Queue 这样，我们常用来管理线程细节的方法和对象在 threading 库是否还存在。\n别担心，都还在，我们仍可以使用它们来实现对线程进行细粒度的控制。而且从 Python 3.2 开始，标准库新增了一种叫做 Executors 的更高等级的抽象，如果你不需要太细粒度的控制，可以用它来为你管理很多细节。\n例子中另一个有趣的改变是，每一个线程都需要创建自己的 requests.Session()。当你看 requests 的文档时，可能看不出来是不是需要这样做，但是读过这篇 issue 之后会发现，似乎确实每个线程都需要有独立的 Session。\n这也是使用 threading 的趣味点以及难点，因为操作系统控制着任务们在中断和启动之间来回切换，所以任何在线程间共享的数据都需要保护，或者是保证线程安全。不巧的是，requests.Session() 并不是线程安全的。\n取决于数据的种类和使用方式的不同，有很多种策略可以保证访问数据时的线程安全。其中一种就是使用线程安全的数据结构，比如 Python 的 queue 模块中的 Queue。\n这些数据结构一般会使用类似 threading.Lock 这种低层级的基元来确保在同一时刻只能有一个线程访问同一块代码或同一段内存。ThreadPoolExecutor 底层就是以这种策略来实现的，直接用它的对象即可。\n例子中也提到了另外一种策略，它被称为线程本地存储。threading.local() 创建了一个类似全局变量的对象，但是对于每个线程来说，这个对象又是专门只服务单个线程的。例子中的 thread_local 和 get_session() 实现了这一点。\nthread_local = threading.local() def get_session(): if not hasattr(thread_local, \u0026#34;session\u0026#34;): thread_local.session = requests.Session() return thread_local.session threading 模块中的 local() 方法专门解决这个问题，虽然看着有些奇怪，但是我们也确实只是需要创建一个对象，而不是为每个线程各创建一个。而且这个对象也会很谨慎地为不同线程隔离不同的数据。\n当 get_session() 函数被调用时，thread_local 会查找到具体是哪个线程正在运行，并为其准备相应的 session。每个线程在第一次调用 get_session() 时都会创建一个 session，然后在接下来的每次调用中都只会使用这一个 session 对象，始终如一。\n最后，稍微提一句线程数量的选择。示例代码中使用了5个线程，你可以随意调整这个数值，然后看看整体时间有什么变化。你可能会想，为每个下载任务都单独设置一个线程应该是最快的，但是并不是如此，至少在我的机器上不是如此。我发现最快记录的线程数大概是在5-10个，而如果数量再增加，线程创建和销毁所付出的额外时间会抵消掉其节省下的时间。\n问题的难点就在这里，对于不同的任务来说，最合适的线程数量并不是一个常量，所以经验也是很重要的说。\n为什么线程版本如此流行 贼快！这里是我测试的最快的结果，还记得之前非并发版本的耗时么，那可是有14秒之多：\n$ ./io_threading.py [most output skipped] Downloaded 160 in 3.7238826751708984 seconds 线程版本的执行时序图如下：\n它启动了多个线程来同时向多个网站发出请求，使得程序可以将等待时间重叠，以至于可以更快的获取最终结果！流弊！目标达成！\n多线程版本的问题 如你所见，为了达到这样的效果，我们稍微增加了一些代码，也确实需要思考一下哪些数据可以在线程间共享。\n线程可能会以一些隐秘又难以追踪的方式进行交互，这些交互就很容易造成“竞态条件”(race conditions)，以至于会频繁产生一些随机的、时有时无的，但又很难找到原因的bug。不熟悉竞态条件这个概念想拓展阅读的可以看下面部分。\n竞态条件 Race Conditions 竞态条件是对于那些在多线程代码中频繁发生的不易察觉的一类bug的统称。造成竞态条件的原因是，开发人员对于数据访问没有做到充分的保护，没有防止线程间的相互干扰。在编写多线程代码时，你需要做一些额外的操作来确保线程安全。\n操作系统会控制你的线程什么时候运行，以及什么时候将它中断然后再换另一个线程开始运行。线程的切换会发生在任何时间点，甚至可能是在执行一段 Python 命令的子步骤时。来看下面这段代码：\nimport concurrent.futures counter = 0 def increment_counter(fake_value): global counter for _ in range(100): counter += 1 if __name__ == \u0026#34;__main__\u0026#34;: fake_data = [x for x in range(5000)] counter = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=5000) as executor: executor.map(increment_counter, fake_data) 这段代码和之前 threading 的例子在结构上非常相似，不同之处在于每个线程都会访问同一个全局变量 counter，同时再让它自增。这个变量没有做任何保护，也就是说它不是线程安全的。\n为了让 counter 自增，每个线程都需要读取它的当前值，然后加一，再将这个值保存回这个变量。这些步骤发生在执行这条语句时：counter += 1。\n操作系统不会关心代码是怎么写的，它会在任何时刻切换线程执行，这就可能导致有时线程切换发生在该线程读取了 counter 值但没将其自增并写回时，如果新的线程又修改了 counter 的值，那么之前那个线程里却保存着 counter 的历史版本，那问题也就随之出现了。\n和你想的一样，出现这种情况的时候非常稀少，可能运行这个程序几千次也看不到一次。这也是这类问题非常难以调试的原因所在，bug相当难复现而且还可能出现随机错误。\n在之前的例子中曾提到过，requests.Session() 不是线程安全的，这也就意味着如果多个线程使用同一个 session，那么类似上面描述的交互bug就可能会在某些地方出现。提到这一点不是因为我想黑一波 requests，而是想指出这些都是难以解决的问题。\nasyncio 异步版本 在开始解释 asyncio 示例代码之前，我们先来了解一下 asyncio 是如何工作的。\nasyncio 基础 尽管这里描述的是一个 asyncio 的简化版本，有很多细节也没有展示，但是仍能表达出它是如何工作的。\n一般来说，我们提到的 asyncio 概念时，指的是一个单独的 Python 对象，这个对象被称为事件循环，控制着每个任务何时运行以及如何运行。事件循环了解每一个任务，也知道它们正处于什么状态。真实情况下，任务可能会有很多状态，但是现在，我们可以想象一个简化版的事件循环，他只支持两种任务状态。\n就绪状态指，任务有工作需要做，而且可以随时开始的状态；等待状态指，任务正在等待一些外部事物的响应，比如网络操作。\n简化版的事件循环会有两个任务列表，一个状态一个。事件循环会挑一个就绪状态的任务让它启动，然后将控制权完全交给这个任务，直到该任务主动将控制权交还给事件循环。\n事件循环重新拥有控制权之后，先会把刚刚交还控制权那个任务放入等待状态任务列表中(译者注：这里查阅资料后发现，Python asyncio 中主动通过 await 交还控制权的任务会从 Running 进入 Pending 也就是等待状态，原文 places that task into either the ready or waiting list 与此不一致，且后文中提到就绪列表中的所有任务都还没有运行，原文也互相矛盾，故改之)；然后事件循环就会遍历等待状态任务列表，看看它们的 I/O 操作有没有完成。没有遍历就绪列表是因为事件循环知道它们都还没有执行过，也就是说就绪列表还保持着之前就绪的状态。\n当所有的任务再一次被分配到正确的列表中之后，事件循环就会挑出下一个任务来运行，简化版事件循环挑选的规则就是，从就绪列表中选一个等了最久的让它运行。整个过程循环往复，至死方休。\nasyncio 中最重要的一点就是，任务永远不会被剥夺控制权，除非它主动放弃。也就是说，它们肯定不会在执行一个操作的途中被打断。这一点也使得 asyncio 在资源共享方面要比 threading 稍微容易一些，毕竟开发人员再也不用担心线程是否安全了。\n以上，从一个高层级的视角展示了 asyncio 的基本理念，如果想对其进行更深入的了解，这篇 StackOverflow 上的回答 提供了些很好的细节，相信对于深挖者会有所帮助。\nasync 和 await 现在，我们来讨论一下添加进 Python 中的两个新的关键词：async 和 await。在上文简要的介绍中，我们看到 await 就像有一种魔力，它能让任务主动将控制权交还给事件循环。当代码中使用 await 来标记一个方法调用时，它就代表着这个调用很可能会花费一些时间，任务也应该从这里放弃控制权。\n简单来说，async 是一个 Python 中的标记，它表明即将要定义的这个函数里会用到 await。尽管在某些情况下，这一点并不完全正确(比如 异步生成器)，但它确实已经 hold 住大部分情况了，在初学者入门时给出一些简单的模型还是绰绰有余的。\n下文中也会展示一种例外：代码中使用了 async with 语句，这条语句会将一个本该使用 await 的对象改为为其创建一个上下文管理器。尽管从语义上看会和之前提到的有些许不同，但本质目的还是一样的：将这个上下文管理器标记为“可以从这里交出控制权”。\n可能你会想，管理事件循环和任务之间的交互也太复杂了。其实对于刚开始接触 asyncio 的开发者来说，上面提到的那些细节其实没那么重要，但有一点必须要牢记：任何一个需要使用到 await 的方法都需要为其标记 async，否则会报语法错误。\n回到代码 现在你应该对 asyncio 有了一些基本的了解，我们可以逐步看看 asyncio 版本的示例代码，研究下它是如何工作的。需要注意的是，示例中使用了 aiohttp，运行代码前记得安装一下，pip install aiohttp。\nimport asyncio import time import aiohttp async def download_site(session, url): async with session.get(url) as response: print(\u0026#34;Read {0} from {1}\u0026#34;.format(response.content_length, url)) async def download_all_sites(sites): async with aiohttp.ClientSession() as session: tasks = [] for url in sites: task = asyncio.ensure_future(download_site(session, url)) tasks.append(task) await asyncio.gather(*tasks, return_exceptions=True) if __name__ == \u0026#34;__main__\u0026#34;: sites = [ \u0026#34;https://www.jython.org\u0026#34;, \u0026#34;http://olympus.realpython.org/dice\u0026#34;, ] * 80 start_time = time.time() asyncio.get_event_loop().run_until_complete(download_all_sites(sites)) duration = time.time() - start_time print(f\u0026#34;Downloaded {len(sites)} sites in {duration} seconds\u0026#34;) 这个版本比之前的两个还要再复杂一点点，虽然结构相似，但是启动任务时会比创建 ThreadPoolExecutor 再多做一些工作。我们从头看下整个示例。\ndownload_site() 最上面的 download_site() 函数和 threading 版本的几乎完全一致，除了在定义函数前需要用 async 标记，以及在调用 session.get() 方法时改为使用 async with 关键词。后面我们会解释为什么这里的 Session 可以直接传递，而不是使用线程本地存储。\ndownload_all_site() 你可以在 download_all_site() 这个函数中看到与 threading 版本相比最大的改动。\nsession 可以在所有任务中共享，因此这里的 session 被创建为了上下文管理器。任务之间可以共享 session 是因为所有任务都运行在同一个线程上，当 session 处于工作状态时，另一个任务不可能将其中断。\n在上下文管理器内，我们使用 asyncio.ensure_future() 创建了一个任务列表，同时这个列表还负责运行这些任务。在所有的任务都已经创建完毕之后，我们调用 asyncio.gather() 来确保 session 上下文能一直存活，直到所有任务全都完成。\n其实 threading 版本的代码也做了类似的工作，只不过细节都放权给 ThreadPoolExecutor 代为处理了。遗憾的是，截止到目前为止还没有出现类似的 AsyncioPoolExecutor 类。\n言归正传，代码的细节中其实还埋藏着一个很小但是很重要的改动。还记得之前我们讨论过确定线程创建数量这档子事么？当时我们的结论是 threading 版本没办法给出一个确定的数量。\nasyncio 很酷的一个优势就在于，它规模的伸缩性要比 threading 好太多。相比于创建线程，创建异步任务花费的资源和消耗的时长要少的多。因此，创建并且运行更多的任务效果也很好。就像示例中那样，我们为每一个网站都创建了一个独立的任务来执行下载，整体效果也是非常好。\n__main__ 最后，asyncio 生态要求我们必须启动一个事件循环，并且告诉它有哪些任务需要运行。示例最下面的 __main__ 代码块中包含了 get_event_loop() 和 run_until_complete() 的代码。该说不说的，Python 的核心开发者们为这些函数做的命名确实是不错。\n如果你升级到了 Python 3.7，Python 的核心开发者们又简化了这些语法，他们将 asyncio.get_event_loop().run_until_complete() 这样的绕口令简化为了 asyncio.run()\n为什么 asyncio 版本如此流行 真的贼快！在我的机器上测试时，这是最快的一个版本，甚至遥遥领先：\n$ ./io_asyncio.py [most output skipped] Downloaded 160 in 2.5727896690368652 seconds 执行时序图看起来和 threading 的也非常相似，仅仅使用了一个线程，就将所有的 I/O 请求都完成了： 遗憾的是，由于缺少一个类似 ThreadPoolExecutor 这样的装饰器，所以相比与 threading 版本在代码上会显得更复杂些。这也意味着要想获得更优秀的性能，我们必须要多做些额外的工作。\n还有一个常见的论调是，开发人员得时刻想着在合适的位置添加 async 和 await 关键词也是一种额外的复杂性。从某种程度上看，这么讲也没毛病。但从反面角度来看，这能强制开发人员思考给定的任务应该在什么时候交出控制权，从而帮助开发人员做出一份更好、更高效的设计。\n规模问题也很突出。在上面 threading 版本的例子中，为每一个网站分配一个线程所消耗的时间远比只用几个线程要多得多。而 asyncio 的例子中有上百个任务，运行起来却一点都没慢。\n异步io版本的问题 关于这一点，确实有几个问题需要明确。首先在第三方库方面，如果想要利用好 asyncio 的全部优势，就需要依赖特定的异步版本的库。你是否在异步版本的代码中使用 requests 库来下载网站，然后发现还是特别慢？这是因为 requests 并不支持异步，没办法告知事件循环它正在阻塞状态。但也不用太过担心于此，随着时间的推移，越来越多的第三方库都开始支持 asyncio 了，这个问题就会慢慢变得不再是个问题了。\n另外一个，也是更不易察觉的问题就是，在协作式多任务处理中，一旦其中有一个任务不配合协作，那么它所有的优势也就都不复存在。代码中任何一个微小的错误都可能造成一个任务长时间的占用控制权不释放，从而导致其他任务被饿死。事件循环主动将其中断也毫无可能，因为任务还没有将控制权交还给事件循环。\n考虑到这一点，我们来介绍另一种完全不同的并发方式，multiprocessing 多进程。\n多进程版本 与之前的方法不同，multiprocessing 版本的代码可以充分的利用多核处理器的性能。来看下代码：\nimport requests import multiprocessing import time session = None def set_global_session(): global session if not session: session = requests.Session() def download_site(url): with session.get(url) as response: name = multiprocessing.current_process().name print(f\u0026#34;{name}:Read {len(response.content)} from {url}\u0026#34;) def download_all_sites(sites): with multiprocessing.Pool(initializer=set_global_session) as pool: pool.map(download_site, sites) if __name__ == \u0026#34;__main__\u0026#34;: sites = [ \u0026#34;https://www.jython.org\u0026#34;, \u0026#34;http://olympus.realpython.org/dice\u0026#34;, ] * 80 start_time = time.time() download_all_sites(sites) duration = time.time() - start_time print(f\u0026#34;Downloaded {len(sites)} in {duration} seconds\u0026#34;) 这段代码看起来可比 asyncio 版本的短多了，而且又和 threading 版本非常的相似，但是在我们深挖代码细节之前，首先来了解下 multiprocessing 做了些什么。\nmultiprocessing 核心思想 在本小节之前的所有关于并发的示例中，它们都是在电脑中的单个 CPU 或核中运行的，原因和 CPython 当前的设计以及一种叫全局解释器锁(GIL)的东西有关。\n本文并不深入讨论 GIL，我们现在仅需知道前文的同步、线程、异步io版本的代码都只能运行在单核上这一点就足够了。\n而标准库中的 multiprocessing 库设计的初衷就在于打破这种壁垒，让代码可以在多个 CPU 上运行。从更高层级来看，它会在其他核上创建新的解释器实例，然后把程序的部分代码外包出去交给新的解释器来做。\n可以想象，创建一个独立的 Python 解释器不会比在当前解释器创建新的线程要快，这是一项重量级的操作，其间也会有一些限制和困难，但是如果用于解决合适的问题，它就会展示出强劲的动力。\nmultiprocessing 代码 如何提升计算密集型程序的速度 同步版本 多线程异步版本 多进程版本 什么时候需要使用并发 结论 ","date":"2024-02-29T17:39:06+08:00","permalink":"https://aiar.site/post/423359792f6842499cae60e1b2595c5a/","title":"[译] 使用并发加速你的程序"},{"content":" 原文标题：What Is the Python Global Interpreter Lock (GIL)?\n原文作者： Abhinav Ajitsaria\n原文链接：https://realpython.com/python-gil/\n简单来说，Python 的全局解释器锁(GIL)是一种互斥元(或锁)，同一时刻它只允许一个线程持有对 Python 解释器的控制权。\n这意味着，在任意一个时间点都只能有一个线程处于执行状态。GIL 带来的影响对于只执行单线程程序的开发者几乎不可见，但是它会成为计算密集型或多线程代码的性能瓶颈。\n由于即使在拥有多个 CPU 的多线程架构中，GIL 也只允许同一时刻仅有一个线程可执行，因此 GIL 已经臭名昭著了。\n在这篇文章中，你将会学习到GIL是如何影响Python程序性能的，以及如何缓解这些影响\nGIL 解决了 Python 中 的什么问题 Python 使用引用计数进行内存管理，这意味着每一个在 Python 中创建的对象都有一个引用数变量，用于保持追踪有多少指向该变量的引用。当这个数量变为零时，该变量占用的内存将被释放。（译者注：详见CPython 的垃圾回收）\n来看一段代码演示下引用计数是如何工作的：\n\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; a = [] \u0026gt;\u0026gt;\u0026gt; b = a \u0026gt;\u0026gt;\u0026gt; sys.getrefcount(a) 3 在上面的例子中，空列表对象 [] 的引用计数是 3。该对象被 a、b 以及传入 sys.getrefcount() 方法的参数所引用。\n回到 GIL：\n问题在于，这个引用计数变量需要避免竞争局面的产生，比如两个线程同时增加或者减少这个值。这种局面一旦出现，轻则造成一部分内存永远不会被释放，重则导致一些还拥有引用的对象的内存被释放。而这可能会导致 Python 程序崩溃，或者其他奇奇怪怪的 bug。\n比较容易想到的解决方案是，为所有跨线程共享的数据结构加锁，这样可以通过保证数据结构修改的一致性，进而保证引用计数变量的安全性。\n但是，为每一个对象或每一组对象加锁意味着会有多个锁同时存在，这将造成另外一个问题——死锁(死锁只会发生在同时存在超过一个锁的情况下)。从另一个角度来看，频繁申请和释放锁资源也会降低程序性能。\nGIL 是添加在解释器自身上的一个简单的锁，它有这样一个规则：执行任何字节码都需要申请解释器锁。这防止了死锁(因为只有一个锁存在)，也不会对性能造成过多的影响。但是它实际上造成任何计算密集型 Python 程序只能是单线程。\n尽管 GIL 也被其他语言的解释器使用，比如 Ruby，但它并不是这个问题唯一的解决方案。一些语言通过使用引用计数之外的方法(如垃圾回收)来避免 GIL 对线程安全内存管理的依赖。\n另一方面，这也意味着这些语言必须通过添加其他性能提升功能（如JIT编辑器）来弥补 GIL 单线程性能优势的损失。\n为什么 GIL 能被选为解决方案 那么，到底为什么这样一个看起来如此不堪的解决方案会被用在 Python 里呢？这是 Python 开发人员的一记昏招么？用 Larry Hastings 的话(译者注：Python的一位核心开发人员和长期使用者)来说，GIL 的设计决策是 Python 在今天如此受欢迎的原因之一。\n在操作系统还没有线程这个概念时，Python 就已经诞生了。为了让开发更加的快捷，Python 以易用作为开发理念，这也使得越来越多的开发者开始使用 Python。\n很多扩展都是针对已有的 C 语言库来编写的，这些库都是 Python 所必需的。为了确保更改的一致性，这些 C 扩展需要一个线程安全的内存管理机制，而这正是 GIL 可以提供的。\nGIL 很容易实现，也很容易添加到 Python 中。由于只需要管理一个锁，它也提升了单线程的程序的性能。\n非线程安全的 C 库变得更容易集成，而 Python 也因为这些 C 扩展，变得更容易被不同社区采用。\n如你所见，GIL 是 CPython 的开发人员在 Python 早期面临这个难题时的实用性解决方案。\n对多线程的 Python 程序的影响 对于一个典型的 Python 程序甚至任何一个相关的计算机程序来说，计算密集型和 I/O 密集型在性能要求上是有区别的。\n计算密集型程序倾向于将 CPU 的运算能力运用到极致，像多维矩阵数学计算、搜索、图像处理等等都属于计算密集型程序。\nI/O 密集型程序是那些需要花费时间等待输入输出的程序，这些输入输出可能来自于用户、文件、数据库、网络等等。I/O 密集型程序有时不得不消耗大量时间用于等待，直到它们从源端取到所需。这是由于在源端输入输出准备好之前，它可能自己也需要做一些处理，比如，用户在思考到底要输入什么指令，或者数据库在查询时运行自己的进程。\n来看一个简单的计算密集型程序，它实现了一个记录倒数时长的功能：\n# single_threaded.py import time COUNT = 50000000 def countdown(n): while n\u0026gt;0: n -= 1 start = time.time() countdown(COUNT) end = time.time() print(\u0026#39;Time taken in seconds -\u0026#39;, end - start) 在我这台 4 核的机器上执行它会有这样的输出：\n$ python single_threaded.py Time taken in seconds - 6.20024037361145 现在，简单调整下代码，使用两个线程分开执行一半的倒数任务：\n# multi_threaded.py import time from threading import Thread COUNT = 50000000 def countdown(n): while n\u0026gt;0: n -= 1 t1 = Thread(target=countdown, args=(COUNT//2,)) t2 = Thread(target=countdown, args=(COUNT//2,)) start = time.time() t1.start() t2.start() t1.join() t2.join() end = time.time() print(\u0026#39;Time taken in seconds -\u0026#39;, end - start) 然后再次执行：\n$ python multi_threaded.py Time taken in seconds - 6.924342632293701 如你所见，两个版本的程序基本花费了相同的时间(甚至多线程的还更长些)。在多线程版本中，GIL 阻止了计算密集线程的并行执行。\nGIL 对于 I/O 密集型的多线程程序在性能方面影响甚微，因为锁在等待 I/O 时是共享的。\n但是那些纯粹的计算密集型程序，比如一个使用多线程将图片分为几部分的程序，不仅会在锁的影响下变成单线程，而且就像上面的例子那样，你还会看到相对于单线程程序，多线程的执行时间还增加了。\n这种时间的增加，就是线程锁在获取和释放时的性能开销所致。\n为什么 GIL 还没有被移除 Python 的开发团队收到过数不胜数的关于 GIL 的抱怨，但是像 Python 这样流行的语言，如果做出像移除 GIL 这样重大的改变，势必会造成一系列向下兼容的问题，而这是 Python 团队无法接受的。\n退一万步说，GIL 完全可以被移除，之前开发人员和研究人员也曾尝试过很多次，但是所有的尝试都破坏了现有的 C 扩展，因为这些扩展都严重依赖 GIL 提供的解决方案。\n当然，是有一些其他的 GIL 替代方案，但是它们有些会降低单线程应用和 I/O 密集型多线程程序 的性能，同时这些程序也会变得异常复杂。毕竟，你也不想在 Python 新版本出来后导致已有的程序效率变慢吧？\nPython 的创始人、BDFL，Guido van Rossum，于 2007 年九月在社区发布的《移除GIL并不容易》一文中做出了一些回应：\n“I’d welcome a set of patches into Py3k only if the performance for a single-threaded program (and for a multi-threaded but I/O-bound program) does not decrease” “只要单线程程序(以及多线程的 I/O 密集型程序)的效率不会下降，我是非常欢迎为 Py3k 附加一些补丁的”\n但自那以后，任何一次尝试都没有满足这个条件。\n为什么 Python 3 也没有移除它 Python 3 确实有机会从头开始开发许多特性，但在这个过程中也破坏了一些现有的 C 扩展，这些扩展需要更新和移植才能与 Python 3 一起工作。这也是在 Python 3 的早期版本社区采用较慢的主要原因。\n但是为啥 GIL 还是没有被移除呢？\n移除 GIL 就会导致新的 Python 3 版本在单线程程序的性能方面相比于 Python 2 竟然会更慢，可想而知结果会是啥。我们确实无法否认 GIL 在单线程程序性能上的优势，因此结果就是 Python 3 中的 GIL 仍然存在。\n但是，Python 3 也确实对现有的 GIL 进行了重大的改进——\n我们之前讨论了 GIL 对于纯计算密集型或纯 I/O 密集型多线程程序的影响，但是，对于那些一部分线程计算密集，另一部分线程 I/O 密集的程序 GIL 带来的影响又会是怎样呢？\n在这样的程序中，Python 的 GIL 会饿死 I/O 密集型的线程，不给它们从计算密集型线程这种获取 GIL 的机会。\n首先我们需要知道 Python 有这样一个内置的机制，当某线程连续使用 GIL 到达一个 固定的时长 后，机制会强制让线程释放 GIL，但如果没有别的线程申请 GIL，则该线程就会继续使用 GIL。\n\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; # The interval is set to 100 instructions: \u0026gt;\u0026gt;\u0026gt; sys.getcheckinterval() 100 但这个机制的问题在于，对于大部分的计算密集型线程而言，它们会抢在别的线程能申请之前重复申请 GIL。这项研究由 David Beazley 整理，可以在 这里 找到可视化的成果。\n这个问题于 2009 年由 Antoine Pitrou 在 Python 3.2 中修复，他添加了一种机制用来监测其他线程请求获取 GIL 但被丢弃的次数，并且在其他线程有机会运行之前，不再允许当前线程重复获取 GIL。\n如何来处理 Python 的 GIL 如果 GIL 正在给你添堵，可以尝试用这些方法解决：\n多进程 vs 多线程： 最受欢迎的方式是使用多进程代替多线程。每个 Python 进程都有它自己的解释器和内存空间，所以 GIL 也就不是问题了。Python内置的 multiprocessing 模块(译者注：原始链接是Python2的，这里的链接换成了最新的Python版本的)可以帮我们很容易的创建进程，就像这样：\nfrom multiprocessing import Pool import time COUNT = 50000000 def countdown(n): while n\u0026gt;0: n -= 1 if __name__ == \u0026#39;__main__\u0026#39;: pool = Pool(processes=2) start = time.time() r1 = pool.apply_async(countdown, [COUNT//2]) r2 = pool.apply_async(countdown, [COUNT//2]) pool.close() pool.join() end = time.time() print(\u0026#39;Time taken in seconds -\u0026#39;, end - start) 在我的机器上执行会有这样的输出：\n$ python multiprocess.py Time taken in seconds - 4.060242414474487 相较于多线程版本，这样能有一个不错的性能提升，还可以吧~\n我们看到，时间花费并没有下降为原本的一半，这是因为进程管理也有它自己的花费。多进程比多线程更重，所以要考虑清楚，这可能会成为新的瓶颈。\n使用其他解释器： Python 有很多解释器的实现版本，最受欢迎的有 CPython、Jython、IronPython 以及 PyPy，它们分别是由 C、Java、C# 和 Python 实现的版本。GIL 仅存在于最初的实现版本 CPython 中。如果你的程序以及它依赖的一些库对于一种或多种解释器都适用，那么你也可以尝试使用别的解释器。\n敬候佳音： 尽管有很多 Python 的使用者在利用 GIL 在单线程程序上的性能优势，但是多线程开发人员也不用太过发愁，因为 Python 社区中一些最聪明的人正致力于将 GIL 从 CPython 中移除，比如 Gilectomy 这个尝试。(译者注：可惜这个尝试已经有8年没有更新了，最后截至到Python3.6版本)\nPython 的 GIL 经常被看作是一个神秘而又困难的主题，但作为 Pythonista，通常你只会在写 C 扩展或者计算密集型程序时，才会被它影响。\n因此，本文为你介绍了 GIL 是什么以及在程序中如何处理它。如果你想继续了解 GIL 底层工作原理，建议观看 David Beazley 的《理解 Python GIL》这场演讲。\n","date":"2024-02-27T10:16:37+08:00","image":"https://s11.ax1x.com/2024/02/28/pFdbDPK.jpg","permalink":"https://aiar.site/post/37e820d83e914ea38386ad1aa323f3c5/","title":"[译] Python全局解释器锁(GIL)是什么"},{"content":" 原文标题：Garbage collector design\n原文作者：Pablo Galindo Salgado\n原文链接：https://devguide.python.org/internals/garbage-collector/\n摘要 引用计数（reference counting）是 CPython 中最主要的垃圾回收算法。它的主要思想是，CPython 会统计每个对象有多少不同的“地方”对该对象有引用。这个“地方”可能是其他对象，可能是全局（或静态）的 C 变量，也可能一些 C 方法中的本地变量。当一个对象的引用数降为 0 时，这个对象就会被回收。如果其中包含一些对其他对象的引用，那么这些“其他对象”的引用数也将减少，若恰好导致某些对象引用数也降为0，那么它们也会被依次回收。可以使用 sys.getrefcount 方法检查对象的引用数，需要注意的一点是，使用这个方法返回的值总会比 1 大，因为在调用这个方法时该方法对于该对象产生一个引用：\n\u0026gt;\u0026gt;\u0026gt; x = object() \u0026gt;\u0026gt;\u0026gt; sys.getrefcount(x) 2 \u0026gt;\u0026gt;\u0026gt; y = x \u0026gt;\u0026gt;\u0026gt; sys.getrefcount(x) 3 \u0026gt;\u0026gt;\u0026gt; del y \u0026gt;\u0026gt;\u0026gt; sys.getrefcount(x) 2 引用计数方案的主要问题在于，它无法处理循环引用。考虑下面的代码：\n\u0026gt;\u0026gt;\u0026gt; container = [] \u0026gt;\u0026gt;\u0026gt; container.append(container) \u0026gt;\u0026gt;\u0026gt; sys.getrefcount(container) 3 \u0026gt;\u0026gt;\u0026gt; del container 在这个例子中，container 存着一个指向自己的引用，因此，即使我们移除了对它的引用（变量“container”），它的引用数也不会变成零，因为在它内部仍有一个引用。因此，仅靠简单的引用计数它将永远无法被清除。正因如此，当一些对象变得无法到达（unreachable）时，我们需要一些额外的机制来清除这些对象间的循环引用。这就是循环式垃圾回收器，通常也叫垃圾回收器（GC）。当然，引用计数也是垃圾回收的一种实现形式。\n内存布局和对象结构 通常情况下，支持 Python 对象的 C 结构大致如此：\nobject -----\u0026gt; +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ \\ | ob_refcnt | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | PyObject_HEAD | *ob_type | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ / | ... | 为了支持 GC，对象的内存布局有了一些更改，它将在通用布局之前存储一些额外的信息：\n+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ \\ | *_gc_next | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | PyGC_Head | *_gc_prev | | object -----\u0026gt; +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ / | ob_refcnt | \\ +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | PyObject_HEAD | *ob_type | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ / | ... | 在这种方式下，这个 C 结构既可以被当作一个普通 Python 对象，也可以在垃圾回收机制触发时，通过对该结构进行简单的类型转换，来访问其前一个内存域来获取垃圾回收相关的额外信息：\n((PyGC_Head *)(the_object)-1)。\n正如后文 优化：复用域以节省内存 部分中解释的那样，这两个附加的内存域通常用来维持一些双向链表，这些链表中存储的对象即是 GC 跟踪着的所有对象（这些链表就是 GC 的各个 代，更多关于详见本文 优化：代 部分）。当然，在 GC 做内存优化时，也不是总会需要完整的双向链表，这时没用到的部分就可以被重用来实现其他目的。\n使用双向链表，是因为它能高效地支持那些 GC 使用最频繁的操作。通常情况下，被 GC 跟踪的对象们会被根据从垃圾回收机制中存活下来的频率收集为多个不相交的集合，收集成的每个集合都被存入独立的双向链表，一个集合被称为一“代”（generation）。在收集期间，每一个“代”又被进一步分为可到达（reachable）和不可到达（unreachable）。双向链表支持内部对象的移动、新增、彻底移除（被 GC 跟踪的对象在 GC 不运行时通常被引用计数回收）以及链表间的合并，所有的这些操作仅仅只需要更新一下几个指针就能完成。需要主义的是，双向链表还支持在其中增加或删除元素时对分区进行迭代，这也是 GC 在运行时经常需要做的。\nGC 提供特定 API 以便对特定对象进行分配、解除分配、初始化、跟踪和解除跟踪。详见 Garbage Collector C API documentation 这篇文档。\n除了上述的对象结构之外，用于支持 GC 对象的 type 对象必须在其 tp_flags 属性中包含 Py_TPFLAGS_HAVE_GC，并提供 tp_traverse 函数的实现。除非可以证明对象不能仅与自身类型的对象形成引用循环，或者除非类型是不可变的，否则还需要提供 tp_clear 函数的实现。\n识别循环引用 CPython 中用来识别引用循环的算法在 gc 模块中有实现。垃圾回收器 仅专注于 清理容器对象（可以容纳一个或多个对象引用的对象）。它们可以是数组、字典、列表、自定义类的实例和拓展模块中的类等。可能有人会觉得循环并不常见，但事实是许多解释器用到的内部引用把循环建的到处都是。比较经典的例子有：\nException 包含 traceback 对象，而 traceback 包含了一个 Exception 自身也在其中的内容列表 模块级函数引用了该模块的字典用来解析全局变量，而这个字典又包含了该模块级函数的条目 实例有对它所属类的引用，所属类又有对其所在模块的引用。同时所在模块又有对它内部所有事物（也可能有其他import进来的模块）的引用，而这会追溯回最开始的那个实例 当表示类似图这样的数据结构时，它的内部节点能链接到自身是再正常不过的一件事 当对象变得不可到达时，要想正确的处理这些对象，首先需要做的就是识别出它们。在循环识别函数的内部有两个双向链表，其中一个包含了所有可以扫描到的对象，另一个包含所有“暂定为”不可到达的对象。\n为了更好的理解这个算法，我们以一个循环链表为例，它有一个有变量 A 引用的链接，还有一个完全不可到达的自引用对象：\n\u0026gt;\u0026gt;\u0026gt; import gc \u0026gt;\u0026gt;\u0026gt; class Link: ... def __init__(self, next_link=None): ... self.next_link = next_link \u0026gt;\u0026gt;\u0026gt; link_3 = Link() \u0026gt;\u0026gt;\u0026gt; link_2 = Link(link_3) \u0026gt;\u0026gt;\u0026gt; link_1 = Link(link_2) \u0026gt;\u0026gt;\u0026gt; link_3.next_link = link_1 \u0026gt;\u0026gt;\u0026gt; A = link_1 \u0026gt;\u0026gt;\u0026gt; del link_1, link_2, link_3 \u0026gt;\u0026gt;\u0026gt; link_4 = Link() \u0026gt;\u0026gt;\u0026gt; link_4.next_link = link_4 \u0026gt;\u0026gt;\u0026gt; del link_4 # Collect the unreachable Link object (and its .__dict__ dict). \u0026gt;\u0026gt;\u0026gt; gc.collect() 2 GC 启动时，会将它所有想扫描的容器对象放入第一个链表，目的是移动所有无法到达对象。由于大部分对象都是可到达的，所以移动不可到达对象显然更加高效，因为这样能更新更少的指针以达到相同的目的。\n每当算法启动时，每个支持垃圾回收的对象都会初始化一个额外的引用计数域，用来存放它被引用的数量（图中的 gc_ref 字段）。这是因为算法需要通过修改引用计数来进行计算，又保证了解释器不用调整对象实际的引用计数。\n然后，GC 会遍历第一个列表中的所有容器，并将容器引用的任何其他对象的 gc_ref 字段数值减一。可以利用容器类中的 tp_traverse 槽（由 C API 实现或者由超类继承）来获取每个容器引用的对象。在扫描完所有对象之后，只有那些引用来自“要扫描的对象”列表之外的对象，其 gc_ref 字段值才会 \u0026gt; 0\n需要注意的是，即便某对象的 gc_ref == 0，也不一定意味着它是不可到达的。这是因为可能会有外部的另一个可访问的对象（gc_ref \u0026gt; 0）对它仍有引用。比如，在我们例子中的 link_2 对象在扫描结束后 gc_ref == 0，但是它仍然在被 link_1 引用，而 link_1 是一个外部可到达的对象。为了获得一个由真正无法到达对象组成的集合，垃圾回收器会使用 tp_traverse 槽再次扫描容器对象，但这次会使用不同的 traverse 方法，将 gc_ref == 0 的对象标记为“暂时不可到达”，然后再将它们移动到暂时不可到达列表中。下图描绘了这样一个状态，GC 已经处理了 link_3 和 link_4 对象，但还没有处理 link_1 和 link_2 对象。\n然后，GC 会接着扫描 link_1 对象，由于 gc_ref == 1 代表它一定可到达（并且它已经在即将成为可访问列表的列表中了），所以 GC 不会做任何事。\n当 GC 遇到一个可到达对象（gc_ref \u0026gt; 0）时，它将使用 tp_traverse 槽遍历该对象的引用，获取该对象所有可到达的对象，再将它们移动到可到达对象列表的末尾（本例中也就是它们初始的位置），最后将它们的 gc_ref 字段值设置为 1。这就是下图中 link_2 和 link_3 的情况，因为它们可以通过 link_1 到达。从上一张图的状态到检查完 link_1 引用的对象之后，GC 知道了 link_3 是可到达的，因此 link_3 被移动回了初始的列表，并且它的 gc_ref 值也被设置为了 1，以便当 GC 再次访问它时能知道它是可到达的。为了避免访问一个对象两遍的问题，GC 会将所有它已经访问过一次的对象做出标记（通过取消 PREV_MARK_COLLECTING 标志），这样，如果一个已经被处理过的对象也被其他对象引用，GC 就不会对它进行二次处理。\n需要注意的是，如果一个对象被标记为“暂时不可达”，后来又被移回可达列表，那么垃圾收集器将再次访问该对象，因为现在该对象的所有引用也都需要处理了。这个过程实际上是对对象图的广度优先搜索。一旦扫描了所有对象，GC 就知道暂定不可达列表中的所有容器对象确实不可达，因此它们就可以被垃圾回收掉了。\n从实际意义上讲，需要注意的是，这些都是不需要递归的，也不会以任何其他方式需要额外线性增长的内存，不管是对象数量、指针数量或指针链长度都不会影响。除了满足 C 需求的 O(1) 存储空间外，对象本身包含了 GC 算法所需的所有存储空间。\n为什么移动不可到达对象会更好 在假定大多数对象都是可以到达的情况下，移动不可到达对象听起来还是挺符合逻辑的。但仔细想想看，这么做值得的原因好像也并不太明显。\n假设我们按顺序创建了 A、B、C 三个对象，那它们在新生代（young generation）中也会以同样的顺序出现。如果 C 指向 B，B 指向 A，然后 C 是外部可到达对象，那么经过算法第一步调整后，A、B、C 的引用计数会变为 0、0、1，因为唯一一个外部可到达的对象是 C。\n然后算法的下一步扫描到 A，A 被移动到暂时不可达列表，同样的事也会发生在 B 身上。接着，当算法扫描到 C 时，B 又被移回了可到达列表。最后，B 又再次被扫描，A 也被移回到了可到达列表。\n所以，虽然看起来结果和最初时完全没有差别，但事实上可到达对象 B 和 A 分别被移动了两次。那为什么这还能算是一次胜利呢？移动可到达对象最直接的算法是将 A、B、C 各移动一次。关键在于，反复横跳的方式把对象的顺序变为 C、B、A - 正好和原始顺序相反。而且在以后的扫描中，它们都不会被移动。由于大部分对象都不是循环的，这可以在无限数量的后续集合中节省无限数量的移动。唯一可能会增加成本的，是第一次的扫描链。\n销毁不可到达对象 一旦 GC 获取到一个确定不可访问对象的列表，它就会启动一个非常脆弱的进程，其目标是完全销毁列表中的对象。这个过程大致来说是按照以下顺序进行：\n处理和清理弱引用（如果有的话）。如果不可达对象集合中的一个元素即将被销毁，但它有些带回调的弱引用，那么这些回调需要被执行。这个过程会非常脆弱，因为任何错误都可能会导致那些处于相悖状态的对象被回调中调用的某些 Python 方法复活或重新可到达。另外，同属于不可到达集合中的弱引用（对象和它的弱引用在同一个不可达的循环中）则需要立即清除，而不需要执行回调。否则它将会在稍后 tp_clear 槽被调用时被触发，造成严重的后果。忽略弱引用的回调没啥大问题，因为对象和弱引用都将会消失，所以说让弱引用先走一步也是合理的。 如果对象有过时的终结器（tp_del 槽），则需要将它们移到 gc.garbage 列表中。 调用终结器（tp_finalize 槽），并将对象标记为已终结，以免因为对象复活或其他终结器先删除该对象时被调用两次。 处理复活的对象。如果某些对象已经复活，GC 将通过再次运行检测算法找到仍然无法访问的新对象子集，并继续处理它们。 调用每个对象的 tp_clear 槽，这样所有内部链接都将被摧毁，引用数降为0，最后触发所有不可达对象的销毁。 优化：代 为了限制每次垃圾回收所花费的时间，GC 使用了一种流行的优化：代。这个概念背后的主要思想是，假设大多数对象的生命周期很短，因此可以在创建后不久就被回收。事实证明，这与很多 Python 程序的现实非常接近，因为许多临时对象的创建和销毁都非常快。对象越老，它就越不可能变得不可访问。利用这一事实，所有的容器对象会被划分为三个空间/代。每个新创建的对象都属于新生代（generation 0）。之前提到的算法将只对特定一代的对象执行，假设 A 对象处在初生代，如果它在这次回收扫描中幸存下来，那么它将被移动到下一代中生代（generation 1），在中生代它被调查的频率将被降低。如果处于中生代的 A 对象在另一轮的 GC 中又幸存了下来，那么它将被移动到最后一代老生代（generation 2），在那里它的被调查频率将被降到最低。\n为了决定垃圾回收运行的时间，收集器会跟踪自上一次收集以来对象分配和回收的数量。当分配的数量减去回收的数量超过 threshold_0 时，将启动回收。最初只检查初生代（generation 0）。如果自检查中生代（generation 1）以来，检查初生代的次数超过了 threshold_1 次，则也检查中生代。对于老生代（generation 2），情况有些复杂；详见下文 回收老生代。刚刚提到的那些阈值可以使用 gc.get_threshold() 函数进行检查：\n\u0026gt;\u0026gt;\u0026gt; import gc \u0026gt;\u0026gt;\u0026gt; gc.get_threshold() (700, 10, 10) 这些代的内容可以使用 gc.get_objects(generation=NUM) 方法来检查，同时可以通过调用 gc.collect(genereation=NUM) 在特定的代中触发收集。\n\u0026gt;\u0026gt;\u0026gt; import gc \u0026gt;\u0026gt;\u0026gt; class MyObj: ... pass ... # Move everything to the last generation so it\u0026#39;s easier to inspect # the younger generations. \u0026gt;\u0026gt;\u0026gt; gc.collect() 0 # Create a reference cycle. \u0026gt;\u0026gt;\u0026gt; x = MyObj() \u0026gt;\u0026gt;\u0026gt; x.self = x # Initially the object is in the youngest generation. \u0026gt;\u0026gt;\u0026gt; gc.get_objects(generation=0) [..., \u0026lt;__main__.MyObj object at 0x7fbcc12a3400\u0026gt;, ...] # After a collection of the youngest generation the object # moves to the next generation. \u0026gt;\u0026gt;\u0026gt; gc.collect(generation=0) 0 \u0026gt;\u0026gt;\u0026gt; gc.get_objects(generation=0) [] \u0026gt;\u0026gt;\u0026gt; gc.get_objects(generation=1) [..., \u0026lt;__main__.MyObj object at 0x7fbcc12a3400\u0026gt;, ...] 回收老生代 除了各种可配置的阈值之外，只有当 long_lived_pending / long_lived_total 的比值高于给定值（固定值为25%）时，GC 才会触发老生代（generation 2）的完整回收流程。原因是，虽然“非完整回收”（即新生代和中生代的收集）需要检查的对象数量大致相同（由上述阈值决定），但完整回收的成本与长寿命对象的总数成正比，而长寿命对象几乎是无限的。确实，有人指出，每次创建 \u0026lt;常量\u0026gt; 对象时都需要进行完整收集，这会导致工作负载的性能急剧下降，因为这些工作负载包括创建和存储大量的长寿命对象（例如，构建一个大的 GC 跟踪列表将显示平方级性能，而不是预期的线性性能）。相反，使用上述比值会在对象总数中产生平摊的线性性能（其效果可以总结为：“随着对象数量的增长，每次完整的垃圾回收的成本越来越高，但我们做的垃圾收集却越来越少”）。\n优化：复用域以节省内存 为了节省内存，支持 GC 的每个对象中的两个链表指针会被重用于多个目的。这也是一种常见的优化，被称为“胖指针”或“标记指针”：携带额外数据的指针。“折叠”到指针中，意味着内联存储在表示地址的数据中，利用内存寻址的某些属性。（译者注：这段没看懂。。）大多数体系结构会将数据的类型与数据的大小对齐（通常是一个字或多个字），这种差异使得指针的一些最低有效位未被使用，这些位就可以用来标记或保存其他信息 - 最常见的是作为位字段（每个位都是一个单独的标记）- 只要使用指针的代码在访问内存之前屏蔽掉这些位。例如，在32位体系结构上（地址和字长），一个字是32位 = 4字节，因此字对齐的地址总是4的倍数，也就是说二进制串都会以00结束，最后两位就可以别做他用了；而在64位体系结构中，一个字是64位 = 8字节，所以对齐的地址总是8的倍数，二进制串都会以000结束，就会有三位可以使用。\nCPython GC 使用两个胖指针，它们对应于上文 内存布局和对象结构 一节中讨论的 PyGC_Head 的额外字段域：\n警告 由于存在额外的信息， “标记”或“胖”指针不能直接解引用，在获取真正的内存地址之前，必须剥离额外信息。对于直接操作链表的函数，需要特别小心，因为这些函数通常假定链表中的指针初一一致状态。\n_gc_prev 字段通常作为“前一个”指针来维护双链表，但其最低的两位用于保存标记 PREV_MASK_COLLECTING 和 _PyGC_PREV_MASK_FINALIZE。在每次回收间隙，唯一可以出现的标志是 _PyGC_PREV_MASK_FINALIZE，他表示对象是否已经被终结。在回收期间，除了这两个标志之外，_gc_prev 还临时用于存储引用计数（gc_ref）的副本，届时 GC 链表会变成单链表，直到 _gc_prev 被恢复。 _gc_next 字段被用作“下一个”指针来维护双链表，但在收集期间，它的最低位用于保存标记 NEXT_MASK_UNREACHABLE，该标记表示在周期检测算法期间对象是否暂时不可达。这是仅用双链表实现分区（译者注：分为可到达和暂时不可达列表，详见上文识别循环引用）的一个缺点：虽然大多数操作的耗时都是恒定的，但是没有有效的方法来确定对象当前在哪个分区中。所以，当需要时，会使用特别的技巧（如 NEXT_MASK_UNREACHABLE）。 优化：延迟追踪容器 某些类型的容器无法产生引用循环，因此不需要由垃圾回收器跟踪。取消对这些对象的跟踪可以减少垃圾回收的成本，但是确定哪些对象可以不跟踪并不是那么容易，这就需要权衡二者利弊。解除对容器对象追踪的时刻，有两种可能的策略：\n在容器对象被创建时 在容器对象被垃圾回收检查时 一般来说，不跟踪原子类型的实例，而跟踪非原子类型的实例（容器、用户定义对象等）。当然，可以提供一些特定类型的优化，来压缩简单实例在垃圾回收时占用的空间。以下是受益于延迟追踪的原生类型示例：\n只包含不可变对象（整数、字符串等，以及递归包含不可变对象的元组）的元组不需要追踪。解释器创建了大量的元组，其中许多元组直到垃圾回收时才会存在。因此，符合这样条件元组，不值得在对象创建时取消追踪。也就是说，除了空元组之外，所有的元组在创建时都会被追踪，后面在垃圾回收期间，再来确定是否可以不追踪幸存的元组（译者注：额，感觉有点车轱辘话）。如果元组的所有内容都没有被追踪，那么该元组可以不被追踪。在每个垃圾回收周期中，检查元组都会被检查是否被追踪。取消元组的追踪可能需要一个周期以上。 只包含不可变对象的字典也不需要被追踪。字典在创建时，不会被追踪。如果有被追踪项被插入到字典中，不管其作为键亦或是值，都会将该字典设置为需要追踪。在完整的垃圾回收期间（所有代），收集器将取消跟踪所有内容都未被追踪的字典。 垃圾回收器模块提供了 Python 函数 is_tracked(obj)，他返回对象当前的跟踪状态。当然，后续的垃圾回收可能会改变这个状态。\n\u0026gt;\u0026gt;\u0026gt; gc.is_tracked(0) False \u0026gt;\u0026gt;\u0026gt; gc.is_tracked(\u0026#34;a\u0026#34;) False \u0026gt;\u0026gt;\u0026gt; gc.is_tracked([]) True \u0026gt;\u0026gt;\u0026gt; gc.is_tracked({}) False \u0026gt;\u0026gt;\u0026gt; gc.is_tracked({\u0026#34;a\u0026#34;: 1}) False \u0026gt;\u0026gt;\u0026gt; gc.is_tracked({\u0026#34;a\u0026#34;: []}) True ","date":"2024-01-12T14:19:45+08:00","image":"https://s11.ax1x.com/2024/01/16/pFFyS10.png","permalink":"https://aiar.site/post/52397c9cfe524c17a37e15b83a022f94/","title":"[译] CPython垃圾回收器的设计"},{"content":"目标 将使用 SASL/SCRAM 认证管理协议进行用户管理，密码采用 SCRAM-SHA-256 作加密处理。中文文档\n通过 ACLs 进行用户权限管理。\nOperation Backup Config $ cd [KAFKA_HOME] $ cp -R config config_bak Start Zookeeper \u0026amp; Kafka If No Start -daemon 为后台守护启动\n$ ./bin/zookeeper-server-start.sh -daemon ./config/zookeeper.properties $ ./bin/kafka-server-start.sh -daemon ./config/server.properties Add Users $ bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config \u0026#39;SCRAM-SHA-256=[password=xG0^qO5\u0026amp;]\u0026#39; --entity-type users --entity-name admin $ bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config \u0026#39;SCRAM-SHA-256=[iterations=8192,password=hL7!rV4^]\u0026#39; --entity-type users --entity-name thtf $ bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type users Configuring Kafka Brokers // kafka-admin-jaas.conf KafkaServer { org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;admin\u0026#34; password=\u0026#34;123456\u0026#34;; }; KafkaClient { org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;admin\u0026#34; password=\u0026#34;123456\u0026#34;; }; Configure Kafka Server SASL Properties $ vi config/server.properties # config/server.properties listeners=SASL_PLAINTEXT://0.0.0.0:9092 advertised.listeners=SASL_PLAINTEXT://192.168.1.237:9092 security.inter.broker.protocol=SASL_PLAINTEXT sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256 sasl.enabled.mechanisms=SCRAM-SHA-256 authorizer.class.name=kafka.security.authorizer.AclAuthorizer super.users=User:admin Configure Kafka Client SASL Properties $ vi config/sasl.conf # config/sasl.conf security.protocol=SASL_PLAINTEXT sasl.mechanism=SCRAM-SHA-256 [Optional] Create Start Shell for Kafka Service with JAAS ** this way is the same as using .env and original start shell**\n$ cp ./bin/kafka-server-start.sh ./bin/kafka-jaas-server-start.sh $ vim ./bin/kafka-jaas-server-start.sh # ./bin/kafka-jaas-server-start.sh # to the end of file export JAAS_CONF=\u0026#34;-Djava.security.auth.login.config=$base_dir/../config/kafka-admin-jaas.conf\u0026#34; exec $base_dir/kafka-run-class.sh $EXTRA_ARGS $JAAS_CONF kafka.Kafka \u0026#34;$@\u0026#34; Control KAFKA_OPTS by .env **Notice: Must register KAFKA_OPTS param like below to ENVIRONMENT if you want to use any operation in ./bin **\n# .env export KAFKA_OPTS=\u0026#34;-Djava.security.auth.login.config=./config/kafka-admin-jaas.conf\u0026#34; Restart Service Notice: Command order can NOT be reversed\n$ cd [KAFKA_HOME] $ ./bin/kafka-server-stop.sh $ ./bin/zookeeper-server-stop.sh $ source .env $ ./bin/zookeeper-server-start.sh -daemon ./config/zookeeper.properties $ ./bin/kafka-server-start.sh -daemon ./config/server.properties Add ACLs for Users Notice: Must set this AFTER configuring Authentication and restarting kafka service\nproducer\n$ bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config config/sasl.conf --add --allow-principal User:test-user --producer --topic test-topic consumer\n$ bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config config/sasl.conf --add --allow-principal User:test-user --consumer --topic test-topic --group test-group ","date":"2024-01-11T10:29:00+08:00","image":"https://s11.ax1x.com/2024/01/11/pF9gP1K.jpg","permalink":"https://aiar.site/post/90ecbc6e08ef41d184ee330d4e7f0b7b/","title":"Kafka 启用用户鉴权功能"},{"content":"注：使用 corntab 进行调度务必请将Kettle自身调度和循环关闭\n1.环境准备 1.1 检查资源库配置文件 .kettle/repositories.xml 存在性 $ cat /home/etl/.kettle/repositories.xml 不存在则以 1.2 中内容模板新建文件\n1.2 检查资源库路径是否符合预期 将 \u0026lt;base_directory\u0026gt;\u0026lt;/base_directory\u0026gt; 标签中的目录位置修改为期望的资源库路径\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;KettleFileRepository\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Resources\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;File repository-Resources\u0026lt;/description\u0026gt; \u0026lt;is_default\u0026gt;true\u0026lt;/is_default\u0026gt; \u0026lt;base_directory\u0026gt;/home/etl/data-integration/Resources\u0026lt;/base_directory\u0026gt; \u0026lt;read_only\u0026gt;N\u0026lt;/read_only\u0026gt; \u0026lt;hides_hidden_files\u0026gt;N\u0026lt;/hides_hidden_files\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; 1.3 上传文件到服务器资源库 将配置好的作业和转换xml文件上传至服务器资源库，例如使用 xftp 等工具\n2.调试配置调度 2.1 测试单次调用 使用 curl 工具通过Kettle自带接口触发作业单次执行。\n$ curl -u {用户名}:{密码} \u0026#34;http://{ip}:{port}/kettle/executeJob/?rep={资源库名称}\u0026amp;job={作业名称}\u0026#34;` 说明：\n用户名和密码是Kettle子服务器的密码 {} 代表替换参数，url中有 \u0026amp; 所以整体需要加上 \u0026quot;\u0026quot; Kettle自带接口的触发机制不在本文讨论范围，只需要知道可以以get请求固定格式url即可执行指定作业就好了 实例：触发执行本机Resources资源库的0494J作业\n$ curl -u cluster:cluster \u0026#34;http://127.0.0.1:8080/kettle/executeJob/?rep=Resources\u0026amp;job=0494J\u0026#34; 这里是curl的用法指南。\n通了会显示如下信息，异常则会显示相应异常信息:\n\u0026lt;webresult\u0026gt; \u0026lt;result\u0026gt;OK\u0026lt;/result\u0026gt; \u0026lt;message\u0026gt;Job started\u0026lt;/message\u0026gt; \u0026lt;id\u0026gt;d996f743-bc27-46da-862b-6ea8be6cf4d2\u0026lt;/id\u0026gt; \u0026lt;/webresult\u0026gt; 2.2 配置循环调度 使用 crontab 工具实现循环调度。\n$ crontab -l # 显示当前用户所有cron任务 $ crontab -e # 编辑修改增删cron任务，编辑器走VISUAL或EDITOR环境变量，可通过 `echo $EDITOR` 命令查看 $ tail -f /var/log/cron # 查看cron任务执行日志 实例：创建一个每5分钟执行一次的调度任务\n$ crontab -e */5 * * * * /usr/bin/curl -u cluster:cluster \u0026#34;http://127.0.0.1:8080/kettle/executeJob/?rep=Resources\u0026amp;job=0494J\u0026#34; 这里是crontab的用法指南。 这里是vim的用法指南。\n","date":"2024-01-11T10:21:10+08:00","image":"https://s11.ax1x.com/2024/01/09/pFp61fK.png","permalink":"https://aiar.site/post/9238c7b48c0d46679242255cb1d09f96/","title":"crontab定时执行Kettle Job"},{"content":"概述 最近在将 Python3.6 升级为 Python3.9，记录下过程。 由于服务器环境均无法连接互联网，故采用从可联网机器上下载安装包再上传至服务器离线编译安装的方式。\n本文默认没有专职服务器运维人员，个人拥有服务器所有权限。（有专业人员哪还用得着自己装。。ORZ）\n下文在 CentOS 7.8 中实测成功。\n依赖 在安装 Python 时，程序会依赖一些系统程序。为了能正确安装，需要提前将这些依赖装好。\n可以在 这里 按照系统版本和所需包搜索下载。\nps: 说实话，其实装的时候还是有点慌的\n以 CentOS 7.8 为例，其他系统版本需编译试错、搜索 编译工具 (使用的是系统自带的)\ngcc == 4.8.5 make == 3.82 系统依赖\nzlib-devel # 影响 zlib 包的安装 libffi-devel # 忘了影响啥，装就对了 openssl-devel # 影响 _ssl 包的安装，如果装了这个还是无法正确安装，则可以看 后文附录部分 bzip2-devel # 影响 _bz2 包的安装 The necessary bits to build these optional modules were not found: _bz2 _curses _curses_panel _dbm _gdbm _hashlib _lzma _sqlite3 _ssl _tkinter _uuid readline zlib _ctypes - libffi-devel _bz2 - bzip2-devel _uuid - uuid-devel/libuuid-devel 前者好像和gdbm-devel有冲突 _lzma - xz-devel _dbm - gdbm-devel zlib - zlib-devel readline - readline-devel _hashlib - openssl11 _ssl - openssl11 下载 Python官方下载地址：https://www.python.org/downloads/\nPython官方 ftp 下载地址：https://www.python.org/ftp/python/\n可以按需下载相应的 Python 版本，由于是要在 Linux 中编译安装，所以我们需要下载 Gzipped source tarball 或者 XZ compressed source tarball，前者是 .tgz 格式，后者是 .tar.xz，正确解压后的结果无本质区别。\n每个大版本中较新的版本建议从 ftp 中下载，downloads 中不是很全。\n需要注意的是，Python 的同一个大版本在绝大多数情况下对于第三方包来说区别不大， 即 Python 3.9.* 中所有的版本，绝大部分情况都可以通用，差别细节详见官网版本介绍。\n解压 下载之后我们就会得到一个类似 Python-3.9.13.tgz 这样的文件，将其上传到服务器之后，随便在一个地方进行解压即可，这个文件可以理解为一个安装包，具体安装到哪里需要后续配置。\n$ cd /path/to/xxx $ tar -xvf Python-3.9.13.tgz 配置 然后进入到解压出的文件夹中，开始配置安装规则。\n常用的参数就是 --prefix 这个参数，它决定了Python将被安装在哪里。 如果不填，将会默认安装到 /usr/local/python 中，如此则后面的编译安装命令需要root权限，但一般 不建议 这样，因为如此可能会导致将操作系统之前已有的 Python 强制覆盖，引发一系列不可控的问题。\n该示例如果完成安装，则 Python 命令的可执行文件位置是 /home/app/depends/python/py39/bin/python3.9\n$ cd Python-3.9.13/ $ ./configure --prefix=/home/app/depends/python/py39 配置成功后，可能会有类似下文这样的建议，但笔者亲测，当 gcc 版本比较低时(比如本例中的4.8.5)，使用该建议无法编译成功。所以建议低版本的 gcc 不要使用 --enable-optimizations 参数。\nIf you want a release build with all stable optimizations active (PGO, etc), please run ./configure --enable-optimizations 编译 配置如果没有问题，就可以开始编译了，如果安装到了系统目录下则需要root权限。\n$ make 如果依赖没有安装，或者遗漏，那么配置、编译这两步可能需要反复几次。 判断自己需要的内置包能不能正常安装，就需要看 make 的执行结果，最后下面一块的内容会有类似下面这样的输出。如果其中没有 Failed 或者 error 等关键词出现，并且 necessary 中缺少的包你未来也用不到，那么恭喜你可以进行下一步 安装 了。\nPython build finished successfully! The necessary bits to build these optional modules were not found: _tkinter To find the necessary bits, look in setup.py in detect_modules() for the module\u0026#39;s name. The following modules found by detect_modules() in setup.py, have been built by the Makefile instead, as configured by the Setup files: _abc atexit pwd time 示例中的这次编译意味着，_tkinter 由于缺少相应的依赖，将不会被安装。搜索后得知，缺少 tkinter 这个包，可以参照上文 依赖 进行安装。\n具体哪些包依赖什么，可以以这样的关键词搜索: [系统版本] [Python版本] [缺失的包名] not found，以 _tkinter 为例，即 CentOS7 Python3 _tkinter not found。 如果是国外或开源的操作系统(RedHat、CentOS、Arch、Ubuntu等)建议使用 Google，无法科学上网则建议使用 bing 国际版。如果是国内自主开发的系统，建议使用bing，且尽可能看官方建议。\n拒绝百度，从我做起。\n安装 一般来说，编译能过，而且不缺依赖，则安装就会比较顺。所以，请尽可能确保编译无误后，最后再执行安装。\n$ make install 没报错就算成功，如果配置时用了 --prefix 参数指定了一个 不在 $PATH 中的地址，则会有下面这样的提示。 意思是，没办法直接用 python 命令直接执行刚刚安装好的程序。\nInstalling collected packages: setuptools, pip WARNING: The scripts pip3 and pip3.9 are installed in \u0026#39;/path/to/python/bin\u0026#39; which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-22.0.4 setuptools-58.1.0 查看 PATH 命令\n$ echo $PATH 如果需要在终端直接执行 python，这里提供两种方法来实现。\n使用软连接 可以使用软连接将刚刚安装的 Python 放到 $PATH 中的某个路径下，类似创建了一个快捷方式。 比如，安装目录为 /new/path/to/python，PATH 中有的路径 /xxx/xxx $ ln -s /new/path/to/python/bin/python3.9 /xxx/xxx/python3.9 $ ln -s /new/path/to/python/bin/pip.9 /xxx/xxx/pip3.9 直接将安装目录放到 PATH 中 假设默认 shell 是 bash，安装目录为 /new/path/to/python。则修改该用户的 .bashrc 文件即可。 $ vim ~/.bashrc ~/.bashrc export PATH=/new/path/to/python/bin:$PATH 保存文件后，执行\n$ source ~/.bashrc 以后该用户就可以直接使用 python3.9 命令了。\n附录 当时的情况是，通过 yum 安装 openssl-devel 之后，在安装 Python3.9 时 ssl 模组仍无法正确安装(猜测是openssl版本过低，时间久远有点忘记细节了)，故使用编译安装的方式，重新装了一个 openssl-1.1.1，再在编译 Python 时使用 --with-openssl 参数，最后 ssl 成功安装。\n下为当时的操作记录.\n615 tar -zxvf openssl-1.1.1t.tar.gz 616 ll 617 cd openssl-1.1.1t/ 618 ll 619 ./Configure -h 620 ./config -h 625 cd /path/xxxxx 626 mkdir openssl 627 pwd 628 cd - 629 ./config --prefix=/path/to/openssl --openssldir=/path/to/openssl 630 make 631 make install 632 cd - 633 ll openssl/ 634 cd - 635 cd .. 636 cd Python-3.9.13/ 646 ./configure --prefix=/path/to/py39 --with-openssl=/path/to/openssl 647 make 648 make install 649 /path/to/python3.9 650 whereis python 664 echo $PATH 670 ln -s /path/to/py39/bin/python3.9 /path/in/$PATH/python3.9 671 ln -s /path/to/py39/bin/pip3.9 /path/in/$PATH/pip3.9 673 python3.9 ","date":"2024-01-11T10:07:40+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShAdH.png","permalink":"https://aiar.site/post/0820636dfb6845c58164439d206e9baf/","title":"在Linux中离线编译安装Python"},{"content":"系统环境 本文依赖机器为，64位 CentOS Linux 7.9.2009，Python 使用 CentOS yum 直接安装的 3.6.8 版本。\nNote: Windows，MacOS 流程大体相近，具体细节或有出入。Python2不在本文讨论范围。 Note: 本文依赖官方pip源，自建pypi的离线模式不在本文讨论范围。 Note: 本文虚拟环境仅使用标准库自带的 venv 模块，第三方的poetry，pyenv等不在本文讨论范围。 Note: 关于项目部署，本文使用直接源码放上去的粗暴方式。打包成可执行文件或者whl安装又或CI/CD等方式不在本文讨论范围。\n虚拟环境 项目必须使用单独 虚拟环境，项目中用到的所有第三方包都需在虚拟环境中安装。目的是：\n项目中安装的依赖不会污染系统环境 项目开发完毕后可以快速整理依赖，方便部署 创建 在项目根目录下利用 python 标准库的 venv 模块创建名为 .venv 的虚拟环境。\n$ mkdir myproject $ cd myproject $ python -m venv .venv $ ls -la total 0 drwxrwxr-x. 3 node0 node0 19 Mar 16 15:54 . drwxrwxr-x. 5 node0 node0 54 Mar 16 15:54 .. drwxrwxr-x. 5 node0 node0 74 Mar 16 15:54 .venv 激活 激活后，命令提示符前面会有虚拟环境标识 (.venv)。下文操作中带有该标记的即标识在虚拟环境中执行，没有该标记则表示在系统真实环境。\n$ source .venv/bin/activate (.venv) $ pip list Package Version ---------- ------- pip 21.3.1 setuptools 39.2.0 开发阶段 ！！！强烈建议！！！ 还是找一个在线的环境进行开发，离线开发实属给自己找罪受。\n依赖安装\u0026amp;升级 (.venv) $ pip install requests # 默认安装该包可用的最新版 (.venv) $ pip install requests==21.2 # 安装指定版本 (.venv) $ pip install --upgrade requests # 升级包到最新版本 [Optional] 升级pip 有时pip版本过低，导致部分第三方包无法安装，可以尝试升级pip版本。\n(.venv) $ pip list pip (9.0.3) setuptools (39.2.0) (.venv) $ python -m pip install --upgrade pip (.venv) $ pip list Package Version ---------- ------- pip 21.3.1 setuptools 39.2.0 部署前的准备 当项目开发基本完毕，即将迁移部署测试时，就体现出虚拟环境的优势所在了。 我们可以直接用pip的freeze功能将所在虚拟环境中所有的包及其版本写入一个文件，后续下载依赖安装包进行离线安装，或者直接在线安装都可以以该文件作为指引，方便的一比。\n导出依赖指示文件 (.venv) $ pip list Package Version ------------------ --------- certifi 2022.12.7 charset-normalizer 2.0.12 idna 3.4 pip 21.3.1 requests 2.27.1 setuptools 39.2.0 urllib3 1.26.15 (.venv) $ pip freeze \u0026gt; requirements.txt (.venv) $ cat requirements.txt certifi==2022.12.7 charset-normalizer==2.0.12 idna==3.4 requests==2.27.1 urllib3==1.26.15 [Optional] 根据依赖指示文件下载对应安装包到指定目录 如果部署的环境无法联网，则需事先将依赖的所有安装包下载好，部署时一并带到新环境。\n(.venv) $ pip download -r requirements.txt -d ./packages (.venv) $ ll packages total 460 -rw-rw-r--. 1 node0 node0 155255 Mar 16 16:27 certifi-2022.12.7-py3-none-any.whl -rw-rw-r--. 1 node0 node0 39623 Mar 16 16:27 charset_normalizer-2.0.12-py3-none-any.whl -rw-rw-r--. 1 node0 node0 61538 Mar 16 16:27 idna-3.4-py3-none-any.whl -rw-rw-r--. 1 node0 node0 63133 Mar 16 16:27 requests-2.27.1-py2.py3-none-any.whl -rw-rw-r--. 1 node0 node0 140881 Mar 16 16:27 urllib3-1.26.15-py2.py3-none-any.whl 项目结构 至此，部署的准备阶段也基本完成，项目的结构基本定型。在省略一些非本文重点讨论的诸如.gitignore，.env或者一些部署自动化shell之后，大概能得到一个类似的项目结构。\nmyproject ├── .venv │ └─ ... ├── packages │ ├── certifi-2022.12.7-py3-none-any.whl │ ├── charset_normalizer-2.0.12-py3-none-any.whl │ ├── idna-3.4-py3-none-any.whl │ ├── requests-2.27.1-py2.py3-none-any.whl │ └── urllib3-1.26.15-py2.py3-none-any.whl ├─ app │ ├── __init__.py │ ├── config.py │ └── main.py ├── README.md └── requirements.txt 可以将其打成一个zip包，方便上传服务器，比如叫 myproject.zip\n开始部署 到了一个新的环境，首先要做到就是将项目压缩包放上去，找个位置解压好后，进到项目目录。然后以前文提到方式创建并激活虚拟环境。\n$ unzip myproject.zip $ cd myproject $ python -m venv .venv $ source .venv/bin/activate (.venv) $ pip list Package Version ---------- ------- pip 21.3.1 setuptools 39.2.0 根据依赖指示文件安装依赖 (.venv) $ pip install -r requirements.txt # 在线直接安装 or (.venv) $ pip install --no-index --find-links=./packages -r ./requirements.txt # 离线使用安装包安装 (.venv) $ pip list Package Version ------------------ --------- certifi 2022.12.7 charset-normalizer 2.0.12 idna 3.4 pip 21.3.1 requests 2.27.1 setuptools 39.2.0 urllib3 1.26.15 [Optional] 部署脚本 部署中的一些操作其实较为繁琐，在理解其流程后，可将部署过程中项目的环境搭建、启动停止等操作封装为shell脚本，以简化操作。\n脚本们可单独放置在一个目录下，比如名为 scripts。可根据实际情况编写符合自身项目的脚本，也无固定模式格式要求，主要为了简化操作。\n本文计划将脚本分为两个，第一个为环境搭建，包括虚拟环境的构建初始化以及依赖的安装，命名为 init-env.sh；第二个为项目服务启停相关，包括状态查询、启动、停止和重启，以项目名称命名 myproject.sh。\n具体代码可参考 graft 中的脚本编写。\n$ mkdir scripts $ cd scripts $ touch init-env.sh myproject.sh 参考 Python 官方的包管理平台 PyPI\n","date":"2024-01-11T10:01:17+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShAdH.png","permalink":"https://aiar.site/post/2a95e2853e784470b1db121a406b90ac/","title":"Python虚拟环境构建与新环境部署"},{"content":" 原文标题：What is doc in Python?\n原文作者：Chris\n原文链接：https://blog.finxter.com/what-is--doc--in-python/\nWhat Python 的每个对象中都有一个叫做 __doc__ 的属性，用来存放该对象的文档信息。比如对于 Dog 类，可以直接调用 Dog.__doc__ 来获取它的文档字符串(docstring)信息。\n可以使用三个引号将字符串包围的方式来定义文档字符串，就像例子中这样：\nclass Dog: \u0026#34;\u0026#34;\u0026#34;你最好的朋友。\u0026#34;\u0026#34;\u0026#34; def do_nothing(self): pass print(Dog.__doc__) # 你最好的朋友。 Python 中万物皆对象，函数也不例外，所以也可以在函数中定义文档字符串：\ndef bark(): \u0026#34;\u0026#34;\u0026#34;汪汪汪\u0026#34;\u0026#34;\u0026#34; pass print(bark.__doc__) # 汪汪汪 注意，如果没有定义文档字符串，那么调用 xxx.__doc__ 时将返回 None。\ndef bark(): pass print(bark.__doc__) # None Why 为什么要使用文档字符串(docstring)呢？\n在代码中定义文档字符串最大的好处在于，可以以编程的方式创建漂亮的文档了。借助类似 Sphinx 这样的工具，为项目创建类似下图这样的文档将变得非常容易，只需要在代码中定义文档字符串，即为 __doc__ 赋值。\nPractice 官方 PEP 标准中定义了很多文档字符串优雅的实践，它们被称为 文档字符串规范(Docstring Conventions)。定义项目中的文档字符串时，请尽可能按照这些规范。下面将列举出规范中最重要的7条：\n所有的模块(module)，函数(function)，方法(method)和类(class)都应该拥有文档字符串 为了一致性原则，请使用 \u0026quot;\u0026quot;\u0026quot;三个双引号\u0026quot;\u0026quot;\u0026quot; 来包围文档字符串 即使文档字符串一行就能写下也应使用三个引号，以便于以后扩展 若无特殊情况，文档字符串前后请不要有空行 描述项目代码的行为时，请使用类似 \u0026quot;\u0026quot;\u0026quot;Do X and return Y.\u0026quot;\u0026quot;\u0026quot; 的主格形式，然后以句点结尾。请 不要使用 类似 \u0026quot;\u0026quot;\u0026quot;Does X and returns Y.\u0026quot;\u0026quot;\u0026quot; 这样的第三人称单数形式 多行文档字符串可以以一句概括开头，然后一个空行，接着是更详细的描述，类似 argument --- name of the person (string) 这样来描述函数或方法的一个参数，每个参数占一行。 多行文档字符串无需另起一行，紧接着引号开始就好，就像这样 \u0026quot;\u0026quot;\u0026quot;Some summary... 如果你是个完美主义者，或是有了中级的代码能力，可以查看官方文档来获取更多的例子。\n","date":"2023-01-30T09:56:47+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShAdH.png","permalink":"https://aiar.site/post/fc3072369de748799c9cad55cb6eda25/","title":"[译] Python中的__doc__是啥"},{"content":" 原文标题：PEP 202 List Comprehensions\n原文作者：Barry Warsaw \u0026lt;barry at python.org\u0026gt;\n原文链接：https://peps.python.org/pep-0202/\n概览 本篇 PEP 描述了一项 Python 语法扩展建议，列表推导式。\n方案 建议列表内元素可以使用 for 和 if 语句进行条件构造（包括相应嵌套格式）。\n基本原理 现今，使用 map()、filter() 配合嵌套循环创建列表的方式大行其道，而列表推导式提供了一种更简便的方式。\n例子 \u0026gt;\u0026gt;\u0026gt; print [i for i in range(10)] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u0026gt;\u0026gt;\u0026gt; print [i for i in range(20) if i%2 == 0] [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] \u0026gt;\u0026gt;\u0026gt; nums = [1, 2, 3, 4] \u0026gt;\u0026gt;\u0026gt; fruit = [\u0026#34;Apples\u0026#34;, \u0026#34;Peaches\u0026#34;, \u0026#34;Pears\u0026#34;, \u0026#34;Bananas\u0026#34;] \u0026gt;\u0026gt;\u0026gt; print [(i, f) for i in nums for f in fruit] [(1, \u0026#39;Apples\u0026#39;), (1, \u0026#39;Peaches\u0026#39;), (1, \u0026#39;Pears\u0026#39;), (1, \u0026#39;Bananas\u0026#39;), (2, \u0026#39;Apples\u0026#39;), (2, \u0026#39;Peaches\u0026#39;), (2, \u0026#39;Pears\u0026#39;), (2, \u0026#39;Bananas\u0026#39;), (3, \u0026#39;Apples\u0026#39;), (3, \u0026#39;Peaches\u0026#39;), (3, \u0026#39;Pears\u0026#39;), (3, \u0026#39;Bananas\u0026#39;), (4, \u0026#39;Apples\u0026#39;), (4, \u0026#39;Peaches\u0026#39;), (4, \u0026#39;Pears\u0026#39;), (4, \u0026#39;Bananas\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; print [(i, f) for i in nums for f in fruit if f[0] == \u0026#34;P\u0026#34;] [(1, \u0026#39;Peaches\u0026#39;), (1, \u0026#39;Pears\u0026#39;), (2, \u0026#39;Peaches\u0026#39;), (2, \u0026#39;Pears\u0026#39;), (3, \u0026#39;Peaches\u0026#39;), (3, \u0026#39;Pears\u0026#39;), (4, \u0026#39;Peaches\u0026#39;), (4, \u0026#39;Pears\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; print [(i, f) for i in nums for f in fruit if f[0] == \u0026#34;P\u0026#34; if i%2 == 1] [(1, \u0026#39;Peaches\u0026#39;), (1, \u0026#39;Pears\u0026#39;), (3, \u0026#39;Peaches\u0026#39;), (3, \u0026#39;Pears\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; print [i for i in zip(nums, fruit) if i[0]%2==0] [(2, \u0026#39;Peaches\u0026#39;), (4, \u0026#39;Bananas\u0026#39;)] 参考实现 列表推导式已成为 Python 语言 2.0 版本的一部分，相关信息已收录至1\nBDFL 声明 请参照以上提出的语法 不支持 [x, y for ...] 格式，应调整为 [(x, y) for ...] 格式 嵌套格式 [... for x ... for y ...] 中，类似普通 for 嵌套循环，最后面的索引最先执行 参考 1 http://docs.python.org/reference/expressions.html#list-displays\n源文件：https://github.com/python/peps/blob/main/pep-0202.txt\n","date":"2023-01-09T17:00:01+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShKQf.png","permalink":"https://aiar.site/post/d034826933334bcea9d37ef90edfe891/","title":"[译] PEP202 - 列表推导式"},{"content":" 你推开了门，挥舞着青春，是一个人\n他关上了门，兜售着伤痕，又是一个人\n烟火进锅，逆流成河，你无处可躲\n黑白如彩色，夜夜笙歌，望梅止渴\n你嫉恶如仇，你困兽之斗，夜风在吼\n你从未看透，你只想感受，慷慨赴死的颤抖\n我的心慌，你的心慌，盛夏冰凉\n春风荡漾，春风荡漾，秋冬无暖阳\n烟火进锅，逆流成河，你无处可躲\n黑白如彩色，夜夜笙歌，望梅止渴\n你看清了梦，你忍住了痛，却哭出声\n你在等着谁，你只要一回，慷慨赴死的机会\n你嫉恶如仇，你困兽之斗，听，夜风在吼\n你从未看透，你只想感受，慷慨赴死的颤抖\n我没学过什么作词作曲，不懂什么精神荒芜，更不在意它想表达什么思想感情。\n但这歌听着就是挺带劲。\n","date":"2022-12-28T14:32:30+08:00","image":"https://s11.ax1x.com/2024/01/08/pFSgUtx.jpg","permalink":"https://aiar.site/post/1e90021ae5314282b83e9eb0da8b24f9/","title":"焰火青年"},{"content":"总要有个树洞来存放自己这些啥布拉吉的转瞬即逝的想法。\n这些年来，这些树洞也是几经辗转，从空间的说说到空间的留言板再到微信朋友圈，从微博大号到网抑云空间再到微博小号，再然后就到了这里。\n不知以后还会迁往何处，但这里大概不会是终点。\n我焯！智！ 人类总会有那么多有的没得想法，这大概是基因决定的。\n《人类简史》中提到，智人之所以能够击败所有对手成为世界上唯一的主宰，很大一部分原因就是可能智人在某一次突变中改变了大脑信息传递或连接的方式，导致智人的认知出现了革命性的变化。\n在认知革命中，智人变得可以幻想出本不存在的东西，比如神明，比如理想，比如画大饼。要知道，在同时期的尼安德特人还只能通过血缘亲眷聚集不足百人时，智人可能通过天马行空的画大饼已经千军万马可以横扫世界了。\n所以呢，我们作为智人的后裔，一天天的想点不着边界的东西，岂不是自古以来的优良传统，万一成真了呢诶嘿。😀\nto be or not to be？ 曾看到过一句话，快乐一般来源于两个方面，创造和毁灭。 把这些奇奇怪怪的想法存放到树洞里，创造的快乐get✅，不开心时把树洞里看着心烦的东西通通删掉，毁灭的快乐get✅\n回想起来，我的各个树洞们不知被我删了多少次。人们都说，成长往往体现在一个人发现自己以前很傻逼的时候。若真如此，看来我真的成长了不少次呢。树洞君，你真的为了我的快乐付出了很多呢。\n反抗不了就接受 其实写树洞更重要的原因大概还是因为太过自矜吧。正常来说，想表达观点，就大大方方说出来；喜欢人家女孩子，就鼓起勇气去表白；觉得自己不开心，就吃喝玩乐找好友相聚诉苦。哪有大老爷们如此扭捏，惺惺作态。\n但就像永远也叫不醒一个装睡的人一样，不愿改正就永远无法改正。呸，回忆和自我反思总会让心情变得很差，这段掐了不播。\n来日方长 不管如何，写树洞大概会陪伴我很久，时不时梳理一下最近的想法，不开心了来发几句牢骚，开心时记录美好瞬间，这就足够了。人总要有一个可以自由宣泄的出口，没必要搞的人尽皆知徒惹人厌，就这样发到博客，有缘而来无缘而去，挺好。\n","date":"2022-12-27T10:49:15+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShmWt.jpg","permalink":"https://aiar.site/post/7ee90efc26174b438e1da515aa7c3c30/","title":"树洞的第n次搬迁感言"},{"content":" 任何值得做的事, 做的糟糕也值得\n原文: 知乎 Xiaoyu Ma 的回答\n概述 Hadoop生态圈中各项基本都是为了处理超过单机尺寸的数据而诞生, 每项工具各有特色各有各的用处, 但互相之间又有重合. 可以把他们比作厨房所需的各种工具, 可以用汤锅直接当作碗来吃饭喝汤, 可以用菜刀来削皮. 虽然奇怪的组合也能工作, 但未必是最佳选择.\nHDFS 大数据, 首先就要存的下海量的数据.\n一台机器的容量不够, 那就多加几台机器, 总能存的下, 但是传统文件系统是单机的, 无法横跨不同机器. 使用多台机器存储就得同时分别管理每台机器的文件系统, 繁琐又费事.\n所以 HDFS(Hadoop Distributed File System) 设计的本质就是为了大量数据可以横跨成百上千台机器, 仅使用一个文件系统来对其进行管理. 极大的减轻了维护的成本, 降低了使用的难度.\n计算引擎 存的下海量数据后, 就会开始考虑如何处理数据.\n虽然 HDFS 可以为我们整体管理不同机器上的数据, 但是这些数据太大了. 一台机器处理成T上P的数据, 慢慢跑也许需要好几天甚至好几周. 对于很多场景, 这种效率是无法忍受的. 比如24小时热搜, 就必须在一天之内处理完毕.\n如果使用多台机器分块处理, 就会面临如何进行工作分配, 机器挂了如何重启相应任务, 机器之间如何信息交互, 最终结果如何整体汇总等等一系列问题.\n而计算引擎的引入正是为了解决这些问题. MapReduce 是第一代计算引擎, Tez/Spark 是第二代.\nMapReduce MapReduce 使用很简化的计算模型, 只有 Map 和 Reduce 两个计算过程(中间使用 Shuffle 串联). 用这个模型已经可以处理大数据领域很大一部分问题了.\n什么是 Map, 什么是 Reduce?\n考虑有一个存储在类似 HDFS 中的巨大文本, 现在要统计其中各个词出现的频次. 于是我们启动了一个 MapReduce 程序.\nMap 阶段, 成百上千台机器读取这个文件的各个部分, 把各自读到的部分分别统计出词频. 每台机器都会产生类似 [('hello', 12110), ('world', 12345), ...] 这样的 Pair. 这些机器被称为 Mapper.\n然后进入 Reduce 阶段, 此时会启动大量的机器将 Map 阶段产生的 Pair 进行分批统计, 最后再进行统一整合, 生成最终的词频统计结果.\nTez/Spark Map + Reduce 的简单模型很是暴力, 虽然好用, 但是很笨重.\n第二代的 Tez 和 Spark 除了内存缓存之类的新功能之外, 本质上是让 Map/Reduce 模型更加的通用化, Map 和 Reduce 之间的界限更加模糊, 数据交换变得更为灵活, 进一步减少了磁盘的读写, 以便于描述更加复杂的算法, 取得更高的吞吐量.\n语言层抽象(Pig/Hive) 有了 MapReduce, Tez 和 Spark 之后, 程序员们发现 MapReduce 的程序写起来真麻烦, 他们希望简化这个过程.\n好比有了汇编语言, 虽然几乎什么都能干了, 但还是觉得繁琐. 所以就希望有个更高级更抽象的语言层, 来描述算法和数据处理流程.\n于是就有了 Pig 和 Hive. Pig 是以接近脚本的方式来描述 MapReduce, 而 Hive 则用的是 SQL.\n它们把脚本和SQL语言翻译成 MapReduce 程序, 丢给计算引擎去计算, 而用户就可以从繁杂的 MapReduce 程序中解放出来, 用更简单更直观的语言去写程序.\n当然, 针对第二代的计算引擎, 也有类似的 Hive on Tez, Hive on Spark 以及 SparkSQL 等等.\n承上启下 以上介绍的基本就是一个数据仓库的基本架构了, 最底层 HDFS 用以存储数据, 中层 MapReduce/Tez/Spark 构成数据计算引擎, 上层 Pig/Hive 提供给用户更简洁的交互接口. 这样基本就解决了中低速数据处理的需求.\n那如果想要更快速呢?\n像微博热搜这种产品, 用户需要的是一个不断变化的榜单, 更新延迟在一分钟之内, 上面的手段就无法胜任了. 于是一种新的计算模式被开发出来, 那就是流计算.\n流计算 Storm 是最流行的流计算平台, 流计算的思路是, 如果要达到更实时的更新, 我为何不在数据流进来的时候就处理了?\n比如还是词频统计, 数据流是一个一个的词, 就让他们一边流过, 我一边就开始统计了.\n流计算很强大, 基本无延迟. 但他的短处是, 不灵活. 你想要统计的东西必须预先知道, 毕竟数据流过了就没了, 没算的东西也就没法补算了. 因此, 它是个很好的东西, 但是无法替代数据仓库和批处理系统.\nKV Store 键值存储这个模块比较独立, 有非常非常多的实现, 比如 HBase, MongoDB 等等等等.\n它的作用就是可以非常快速的通过 Key 来找到对应的 Value. 比如用身份证号取到身份数据. 当然这个 MapReduce 来实现, 但是很可能要扫描整个数据集. 而 KV Store 专用来处理这个操作, 所有存和取的操作都专门为此优化了, 从几个P的数据中查找一个身份证号, 可能只需要零点几秒.\nKV Store 的基本理念是, 基本无法处理复杂计算, 大多没法 JOIN, 也许没法聚合, 没有强一致性保护, 但就是快, 极快.\n其他 除了以上这些, 还有些更为特制的系统/组件. 比如 Mahout 是分布式机器学习库, Protobuf 是数据交换的编码和库, Zookeeper 是高一致性的分布存取协同系统, 等等.\n调度系统 有了这么多乱七八糟的工具, 都在一个集群上运转, 大家都需要相互配合有序工作, 所以调度组件就十分重要. 现在最流行的就是 Yarn.\n","date":"2022-06-15T16:41:06+08:00","image":"https://s11.ax1x.com/2024/01/08/pFSg01O.png","permalink":"https://aiar.site/post/ace913866d0649cc9a8f2940214f74c7/","title":"Hadoop生态圈"},{"content":" 万物皆有裂痕，那是光照进来的方向。\n问题描述 某一次重启之后, 触摸板无法 tap to click, 无法触发三指四指的手势事件, 一指两指滑动功能还可以使用。\ngnome-control-center touchpad 设置消失, gnome-tweek 设置 touchpad 无效, dconf-editor 设置 touchpad 无效。\n重启之前的操作可能与触摸板相关的有:\n安装了 todesk 远程控制软件，可能会重载触摸板的配置 sudo pacman -Syu 目前只想到这两个, 不过确实很久没有关机了, 大概有一周左右, 期间滚动升级了很多次, 不过都没有重启或关机\n猜测原因 滚动更新可能触摸板依赖默认安装了 synatics 导致和 gnome 不兼容, Arch Wiki 里这样写着:\nNote: The synaptics driver is not supported by GNOME. Instead, you should use libinput. See this bug report.\n或者也可能是由于 todesk 重载触摸板配置啥的我也不太清楚\n解决方案 前置条件 默认系统已经安装类似 yay 的 AUR 包管理 默认桌面系统是Gnome, 且已可以安装插件 具体操作 remove xf86-input-synaptics package $ sudo pacman -R xf86-input-synaptics install xf86-input-libinput $ sudo pacman -Syu xf86-input-libinput reboot $ reboot 重启之后大概是可以触发 touch to click 了, 如果三指手势仍无法触发, 则进行下面几步\ninstall touchegg $ yay -Syu touchegg enable touchegg $ sudo systemctl enable --now touchegg install gnome extension x11-gestures\n安装该插件即可\nhttps://extensions.gnome.org/extension/4033/x11-gestures/\n一般来说装好插件就三指手势已经可以触发了, 不行的话可以重启试试看\n参考文档 https://bbs.archlinux.org/viewtopic.php?pid=1638596#p1638596\nhttps://itsfoss.com/three-finger-swipe-gnome/\nhttps://wiki.archlinux.org/title/GNOME#Mouse_and_touchpad\nhttps://bugzilla.gnome.org/show_bug.cgi?id=764257#c12\n","date":"2022-02-21T17:22:12+08:00","image":"https://s11.ax1x.com/2024/01/08/pFSgY7R.jpg","permalink":"https://aiar.site/post/e93e60bf2eca473da47d60de30ebcdba/","title":"Arch+Gnome 触摸板功能忽然不全了怎么办"},{"content":" 原文标题：PEP 289 - Generator Expressions\n原文作者：python at rcn.com (Raymond Hettinger)\n原文链接：https://peps.python.org/pep-0289/\n概览 本篇 PEP 介绍了生成器表达式(Generator Expressions)这种高性能的，内存高效泛化的列表推导式(List Comprehensions)和生成器(generators)。\n基本原理 从以往编程经验来看，列表推导式在 Python 的各个角落都有广泛的实用性。但是，其中不少的情况其实并不需要在内存中创建完整的列表(List)，而一次迭代一个元素就恰好满足它们的需求。\n例如，下面这段列表求和的代码将在内存中完整的创建一个乘方的列表，然后遍历其中每一个值，最后当该引用(reference)不再需要时，删除整个列表：\nsum([x * x for x in range(10)]) 现在，可以通过使用生成器表达式来节省内存：\nsum(x * x for x in range(10)) 其他容器对象的构造函数也同样支持类似的特性：\ns = set(word for line in page for word in line.split()) d = dict((k, func(k)) for k in keylist) 生成器表达式对于类似 sum()，min()， max() 这种能将一个可迭代的输入汇聚成一个值的函数有着非常高效的作用：\nmax(len(line) for line in file if line.strip()) 生成器表达式简化(address)了一些使用 lambda 函数的例子：\nreduce(lambda s,.a: s + a.myattr, data, 0) reduce(lambda s,.a: s + a[3], data, 0) 它们可以被简化为：\nsum(a.myattr for a in data) sum(a[3] for a in data) 列表生成式极大减少了开发人员对 filter() 和 map() 的需求；同时，生成器表达式也被寄予厚望来最大可能的减少人们对 itertools.ifliter() 和 itertools.imap() 的使用需求。相比之下，itertools 中其他方法的能力将被生成器表达式进一步增强：\ndotproduct = sum(x * y for x, y in itertools.izip(x_vector, y_vector)) 在升级扩展应用时，与列表推导式类似的语法也能更容易的将已有代码转换为生成器表达式。\n早期的版本中，生成器表达式相比于列表推导式有着相当明显的性能优势。但是后者针对 Py2.4 做了高度优化，现在二者在处理中小型数据集时的性能已经大致相当。\n但随着数据量的增长，由于生成器表达式不会耗尽缓存内存，同时还允许 Python 在迭代之间复用对象，所以生成器表达式往往能展现出更好的性能。\nBDFL 声明 本 PEP 已被 Py2.4 接受。[1]\n细节描述 （也许对于火星读者来说，下面这些说的可能没那么全面确切，但我相信这些例子已经足够表述我的想法，以便在 c.l.py 进行讨论(译者注：最后半句没看懂)。Python 参考手册应涵盖以下所有语义语法规格。）\n生成器表达式的语义等同于创建一个匿名生成器函数并调用它。例如： \u0026gt;\u0026gt;\u0026gt; g = (x**2 for x in range(10)) \u0026gt;\u0026gt;\u0026gt; print g.next() 这等同于：\n\u0026gt;\u0026gt;\u0026gt; def __gen(bound_exp): ... for var1 in bound_exp: ... if exp2: ... for var2 in exp3: ... if exp4: ... yield tgtexp \u0026gt;\u0026gt;\u0026gt; g = __gen(iter(exp1)) \u0026gt;\u0026gt;\u0026gt; del __gen 句法要求，生成器表达式需要直接置于一组小括号内，并且两边都不能有逗号。参考 CVS 中的 Grammar/Grammar 文件，两条规则更改(译者注：语法文件可参照官方文档)： 规则1 atom: \u0026#39;(\u0026#39; [testlist] \u0026#39;)\u0026#39; 更改为：\natom: \u0026#39;(\u0026#39; [testlist_gexp] \u0026#39;)\u0026#39; 其中 testlist_gexp 和 listmaker 基本没差，但 testlist_gexp 只允许在 for ... in 之间进行单个测试。\ntestlist_gexp: test ( gen_for | (\u0026#39;,\u0026#39; test)* [\u0026#39;,\u0026#39;] ) 规则2，参数列表的规则需要进行类似修改。 也就意味着，只有一个参数时，生成器表达式小括号可以省略，如：\n\u0026gt;\u0026gt;\u0026gt; sum(x**2 for x in range(10)) 但其他情况下，表达式小括号必须填写：\n\u0026gt;\u0026gt;\u0026gt; reduce(operator.add, (x**2 for x in range(10))) \u0026gt;\u0026gt;\u0026gt; g = (x**2 for x in range(10)) 确切的细节已签入 Grammar/Grammar 1.49版。\n如果将一个或一组简单变量作为循环变量，那么它(们)不会暴露给外层函数。这样一来不仅有助于开发编码，而且可以让典型用例更加可信赖。在Python的一些未来版本中，列表推导式的迭代变量也将对外层函数代码隐身（并且在Py2.4中，访问迭代变量将会触发Warning）。例如： \u0026gt;\u0026gt;\u0026gt; x = \u0026#39;hello\u0026#39; \u0026gt;\u0026gt;\u0026gt; y = list(x for x in \u0026#39;abc\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print x hello # 而不是 c 列表推导式语法将保持不变，例如： \u0026gt;\u0026gt;\u0026gt; [x for x in S] # 这是一个列表推导式 \u0026gt;\u0026gt;\u0026gt; [(x for x in S)] # 这是一个列表，里面包含了一个生成器表达式 不幸的是，目前二者有着轻微的句法差异，列表推导式：\n\u0026gt;\u0026gt;\u0026gt; [x for x in 1, 2, 3] # python3 已不支持该写法 是合法的，它等同于：\n\u0026gt;\u0026gt;\u0026gt; [x for x in (1, 2, 3)] 但是生成器表达式不支持以上形式：\n\u0026gt;\u0026gt;\u0026gt; (x for x in 1, 2, 3) 非法。\n之前的列表推导式语法将在 Python 3.0 中不再支持，同时也会在 Python2.4 及以后版本中被标为弃用。\n为了让 Python3.0 中列表推导式的语义定义等同于 list(\u0026lt;generator expression\u0026gt;)，上面提到的列表推导式会将其迭代变量“泄漏”到外层环境中的问题，也将在 Python 3.0 中得到修复。同时，在Py2.4 及以后版本中，如果列表推导式的迭代变量和当前上下文中的变量重名时，解释器将触发弃用警告。\n早期绑定 vs. 后期绑定 待续\n名词解释 [1] BDFL 终身仁慈独裁者（英语：Benevolent Dictator For Life，缩写BDFL）是少数开源软件开发者所拥有的头衔。他们通常是某一项目的创始人，并在该项目社区出现争议时拥有最终的决定权。来自维基百科\n","date":"2021-05-21T09:03:49+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShKQf.png","permalink":"https://aiar.site/post/58ee3468894b4f4e8eb1c7b32e008251/","title":"[译] PEP289 - 生成式表达式"},{"content":" 原文: What is Arch User Repository (AUR)? How to Use AUR on Arch and Manjaro Linux?\n最近更新: 2020/9/18 - Dimitrios Savvopoulos\n如果你用过 Arch Linux 或者其他 基于 Arch 的 Linux 发行版(比如 Manjaro)，那么可能曾遇到过 AUR 这个术语。在你尝试安装一个软件的时候，也许有人会建议你从 AUR 来安装它，然后你满脸问号。\n什么是 AUR？为啥要用它？又要咋用？接下来，我将回答这些问题。\n什么是 AUR AUR 是 Arch User Repository (Arch 用户资料库) 的缩写。这是一个 社区驱动的资料库，它专注于服务那些使用 Arch 或者 Arch-base Linux 发行版的用户。AUR 包含一个名叫 PKGBULIDs 的软件包说明(package descriptions)，它可以让你通过 makepkg 来编译你所需的软件包，然后使用 pacman(Arch Linux 的软件包管理器)来安装这个软件包。\nAUR 的创立是为了组织和分享那些来自社区的新软件包，以便于加速受欢迎的软件包被纳入社区资料库。\n有相当一部分进入官方资料库的新软件包开始时都是来源于 AUR。在 AUR，用户可以建设他们自己的软件包建设器(package builds，PKGBUILD 和相关文件)。\nAUR 社区支持为社区中的软件包投票，如果某一个包足够受欢迎(假设它有兼容的许可证和良好的包装技术)，那么它可能可以直接进入社区资料库，这样就可以直接由 pacman 直接访问了。\n简而言之，AUR 就是一种特殊的软件安装途径，它允许开发人员在软件未正式纳入 Arch 资料库时，将该软件提供给 Arch Linux 用户以供安装使用。\n应该使用 AUR 么？有什么风险？ 使用 AUR 就像横穿马路，如果小心谨慎，那就没什么问题。\n如果你是一个 Linux 新手，建议还是不要轻易使用 AUR，直到你已经全面的了解了 Arch/Manjaro 和 Linux 的基础知识。(译者注: 个人感觉 AUR 类似 GitHub)\n的确，任何人都可以向 AUR 上传软件包，但是 Trusted Users(TUs) 会负责密切关注上传的内容。尽管 TUs 会对上传的文件进行质量管控，但是仍无法保证 AUR 中的软件包格式正确或者无危胁。\n在实践过程中，AUR 看起来仿佛很安全，但是理论上它是可以造成一些损害的，当然，这只会发生在你疏忽大意的时候。毕竟，从 AUR 安装软件时，机智的 Arch 使用者们应该每次都会检查 PKGBUILDs 和 *.install 文件。\n另外，如果 AUR 中的软件包被纳入了 core/extra/community，那么 TUs(Trusted Users)会将 AUR 中删除该包，因此它们之间不应存在命名冲突。AUR 中经常包含着开发中的软件包(cvs/svn/git/etc)，他们将会被重命名，比如 foo-git。\n对于 AUR 中的软件包，pacman 会处理其依赖关系并检测文件冲突，因此不必担心某个包中的文件会将另外一个包的文件覆盖掉。除非你在默认情况里添加了 -force 选项，如果你真这么做了，可能会遇到一堆比文件冲突更严重的问题。\n如何使用 AUR 使用 AUR 最简单的方法就是通过一个 AUR helper。大部分的 AUR helper 是命令行工具，有些也支持 GUI 图形化操作。这种工具支持搜索和安装那些发布在 AUR 上的软件包。\n在 Arch Linux 上安装一个 AUR helper 假设你想使用的是 Yay AUR helper 这款工具。首先需要确保你的 Linux 上已经安装了 git。然后 clone 这个仓库，随后进入该文件夹，最后执行安装。\n$ sudo pacman -S git $ git clone https://aur.archlinux.org/yay.git $ cd yay $ makepkg -si 安装完毕后，就可以像下面这样使用 yay 命令安装软件了~\n$ yay -S package_name 当然，也不是说只有使用 AUR helper 才可以从 AUR 安装软件，下一部分你将看到如何不用 AUR helper 使用 AUR。\n不用 AUR helper 安装 AUR 软件包 如果你不想使用 AUR helper，你也可以自行从 AUR 安装软件包。\n建议，在 AUR page 中找到想安装的软件包之后，请参照该软件的 Licence，Popularity，Last Updated，Dependencies 等指标，判断其内容质量。\n$ git clone [package URL] $ cd [package name] $ makepkg -si 假设，你想安装 telegram desktop 这个软件：\n$ git clone https://aur.archlinux.org/telegram-desktop-git.git $ cd telegram-desktop-git $ makepkg -si 在 Manjaro Linux 中启用 AUR 支持 Manjaro Linux 默认不启用 AUR，需要使用 pamac 来启用这一功能，我的笔记本使用的是 Manjaro Cinnamon，但下面的步骤适用于所有版本的 Manjaro。\n打开 Pamac 也就是 Add/Remove Software： 打开 Pamac 之后，进入属性 Preferences： 打开属性对话框，进入 AUR 页签，启用 AUR 支持，启用检测更新，然后关闭对话框。\n现在，搜索软件的时候可以搜到源自 AUR 的软件包了，可以通过软件描述下面的标签来区分。\nAUR 是大家 热爱 Arch Linux 的众多原因 之一，你可以看到它为什么如此流行。\n希望本文对你有用。\n希望能看到各大社交媒体上即将出现的 Arch 主题~\n","date":"2021-05-21T09:01:54+08:00","image":"https://s11.ax1x.com/2024/01/08/pFSgJB9.jpg","permalink":"https://aiar.site/post/8cf42649f267457096df789ace7e2460/","title":"[译] 什么是AUR，如何在Arch和Manjaro中使用AUR"},{"content":" 原文地址：Pattern: API Gateway / Backends for Frontends 原文作者：Chris Richardson 译文出自：Microservice Architecture 译者：Arrackisarookie 情景 试想，你的团队正在搭建一个在线商城，它使用了微服务架构模式，现在需要你实现产品的信息细节页面。针对这个产品信息页面，你需要开发多个版本的用户接口：\n基于 HTML5/JavaScript 适用于桌面和手机浏览器的用户接口 - HTML 由服务器端的 Web 应用程序生成 本地的 Android 和 iPhone 客户端 - 这些客户端通过 REST APIs 与服务器进行交互 另外，这个在线商城必须可以通过一个 REST API 暴露产品信息，以供第三方应用使用获取。\n产品信息页会展示大量有关该产品的信息。比如 Amzon.com 上 POJOs in Action 这本书的信息页展示了：\n这本书的基本信息，像标题，作者，价格等等 你对于这本书的支付历史 是否可以购买(库存) 购买选项 和这本书经常一起购买的书 买了这本书的其他买家还买了哪些书 买家评论 卖家排行 \u0026hellip; 由于这个在线商城使用了微服务架构模式，产品的具体信息被分布在了多个微服务上。比如：\n产品信息服务 - 该产品的基本信息，类似标题，作者 价格服务 - 产品的定价 订单服务 - 产品的支付历史 库存服务 - 产品是否可购买 评论服务 - 买家评论 这样一来，展示产品信息的代码需要从以上所有服务中获取信息。\n问题 一个基于微服务的应用程序客户端如何访问一个个独立的服务呢？\n痛点 由微服务提供的 APIs，它的粒度经常和客户端所需的粒度有所不同。微服务一般会提供细粒度的 APIs，这也就意味着客户端需要和很多微服务产生交互。就像上面情境中描述的，客户端如果需要展示的产品信息细节，就需要从大量的服务中获取数据 不同的客户端需要不同的数据。比如，桌面浏览器版本的产品信息页要比手机版本的更加细致丰富。 对于不同类型的客户端，网络性能也是不同的。比如，手机网络一般要比非手机网络更慢而且延时更高。当然，广域网也会比局域网慢很多。这也就意味着使用手机网络的移动客户端和使用局域网的服务器端 Web 应用程序将有着差异非常大的性能特点。服务器端的 Web 应用程序可以同时处理多个传到后端的请求，并且不会影响到用户的体验，而手机客户端只能做到一点点 服务实例的数量和他们地址(主机地址+端口)的动态改变 拆分成的多个服务可能会随着时间改变拆分方式，这些对于客户端应该是隐藏的 众多的服务可能会使用各种各样的协议，其中有些可能对网络不友好 解决 实现一个 API 网关，让它作为所有的客户端访问服务器的唯一入口。API 网关处理请求一般有两种方式，一些请求会被简单的代理到或路由到合适的服务，而对于另外一些服务，网关可能会同时分发给多个服务(fan out to multiple services)。\n不同于提供一套适用于所有类型的通用 API，我们的 API 网关可以为每一种客户端暴露不同的 API 接口。比如，Netflix API 运行着一套客户端识别适配代码，它可以为每一种客户端提供其所需的最合适的 API 接口。\nAPI 网关可能也会实现安全措施，例如验证客户端是否被授权可以执行该请求。\n变种：服务于前端的后端(Backends for frontends - BFF) 上面所说的这种模式有个变种形式，也就是 BFF 模式。它为每个类型的客户端定义了一套专门的 API 接口。\n在这个例子中，有三种客户端：Web 应用，移动应用和外部第三方应用。同样，也有三种不同的 API 网关，它们和之前提到的三种客户端一一对应。\n结论 使用 API 网关有以下优势：\n客户端将与应用后台如何划分微服务完全隔离 客户端将与决定服务实例位置的问题完全隔离 为每一种客户端提供最佳的 API 削减了大量的请求和往返次数。比如，API 网关允许客户端在一次往返中从多个服务中获取数据。更少的请求次数也意味着更少的资源开销，和更优质的用户体验。API 网关对于移动应用来说十分必要 简化了客户端。改变了客户端的运行逻辑，客户端不再需要调用多个服务，而是把这一切工作移给了 API 网关 它可以将标准的公共网络友好的 API 协议转换成内部使用的任意协议 API 网关也有它的劣势：\n复杂性提升了 - API 网关事实上是一个独立的可插拔部件，它也必须需要开发，发布和管理 增加了响应时间 - 由于经过 API 网关时增加了额外的网络跳转，所以响应时间会有所增加。但是对于大部分应用来说，这一点增加的开销是微不足道的。 议题\n如何实现 API 网关呢？如果该网关必须按照比例缩放以应对高负载，那么最好采用事件驱动或响应式的方法。在 JVM，或者像 Netty，Spring Reactor 这样的基于 NIO 的库效果很好。NodeJS 也是一种选择。 相关的模式 微服务架构模式(Microservice architecture pattern) 为 API 网关模式创造了需求 API 网关必须使用 客户端探索模式(Client-side Discovery pattern) 或者 服务器端探索模式(Server-side Discovery pattern) 来将请求路由到可用的服务实例 API 网关可以验证用户身份，并且会将一个包含用户信息的 访问令牌(Access Token) 传送给服务 API 网关需要使用 断路器模式(Circuit Breaker) 来调用服务 API 网关经常会实现 API 组合模式(API Composition pattern) 已知在使用的 Netflix API 网关 应用范例 请参阅微服务模式中 应用程序范例 的 API 网关部分。它使用 Spring Cloud Gateway 实现。\n","date":"2020-09-03T14:50:29+08:00","image":"https://s11.ax1x.com/2024/01/08/pFSgNA1.png","permalink":"https://aiar.site/post/2b9443a5b0dc404082f04cd7a6802814/","title":"[译] 模式：API网关和BFF"},{"content":"原文链接: Command line productivity with Fish shell\n用 Fish 终端处理事务， 你将拥有不止一种解决问题的途径， 这是她的魅力所在。 无止境的定制方案和调整插件可以让事务更快速， 更高效的被处理完成。\n在使用 WSL 的时候， 我心爱的 zsh 和 oh-my-zsh 组合让我陷入了性能危机。 彷徨无助了一个小时后， 我迷迷糊糊的投入了 Fish Shell 的怀抱， 听说在 WSL 中，Fish 要比 zsh 快十倍 (不是真实的比例指标，但是赶脚真的非常非常快)。 但是，Fish 也有些她自己的怪癖，后面我会介绍到。\n安装 Fish Shell 如果你使用的是 Debian-based 的发行版：\n$ sudo apt-get install fish 如果你使用的是其他平台，请按照 这里 的指示进行操作。\nFish Shell 介绍 Fish Shell 非常的轻量，反馈表现迅速，并且有着丰富的特性。 这意味着，你只需花费一点精力就能在终端中产生可观的生产效率。 主要特性功能有：\n语法高亮 自动建议命令 tab 补全 方法自动导入 Fish 的官方文档非常丰富和详尽，在本地有一份文档的副本， 你可以通过在 Fish Shell 中键入 help 来在浏览器打开这个文档。 对于大多数人来说，安装原生 Fish 就已经足够。 但我们也可以使用插件，主题布拉布拉来使得 Fish 更加可用，和易于调整。 和其他 Shell 类似，Fish 有着大量的插件安装框架， 而我们将使用的插件安装框架被称为 oh-my-fish(omf)。\nomf 介绍 omf 是在 Fish Shell 之外，最上面薄薄的一个层， 因此你不必担心速度和性能的问题。可以通过一条简单的命令安装它：\n$ curl -L https://get.oh-my.fish | fish 安装完毕后，你的 Fish Shell 将获得一个 omf 命令， 这个命令可以安装主题和其他有用的插件。omf 十分直观， 如果你曾使用过 nvm 或者 pip，你会有种似曾相识的感觉。\nomf 主题 omf 有着各种各样的主题供你选择。你可以找到托管在 omf 的所有主题， 或者你可以使用 omf theme 命令来列出所有可用主题， 已安装主题和默认主题。\n安装新主题时，当前 Fish Shell 客户端将会直接应用该主题。\n$ omf install \u0026lt;theme-name\u0026gt; $ omf install bira 列出主题和切换主题\n如果你拥有很多主题，可以这样来在它们之间切换：\n$ omf theme \u0026lt;theme-name\u0026gt; omf 别名 当你需要完成一些重复的任务时， omf 的别名功能可以让你尽可能的减少键盘敲击次数。 Fish Shell 的 alias 命令可以用来定义操作的别名。 你可以通过命令行轻易的使用它。\n$ alias \u0026lt;alias\u0026gt; \u0026#39;\u0026lt;command\u0026gt;\u0026#39; -s $ alias install \u0026#39;sudo apt-get install\u0026#39; -s $ alias remove \u0026#39;sudo apt-get remove --purge\u0026#39; -s 定义了以上的别名后，你就可以通过 install 命令来安装所有的包， 或者使用 remove 命令来完全删除某个包。\n$ install vim $ remove python2.7 创建别名install\n通过 alias 命令创建的别名可以一直保持到本次会话结束， 也就意味着，如果你打开了一个新的终端进程，之前创建的别名将不再工作。\n如果想让之前定义的别名可以在新的终端工作，我们需要使用 -s 标记。 它将在后台使用 funcsave。\n使用了 -s 标记的别名定义，将被定义为永久性的别名， 本机的任何 Fish 终端都可以使用。\n当你忘记了自己定义过啥别名， 你可以使用 alias 命令来浏览所有已定义的别名。 列出所有别名\n使用 nvm Fish Shell 的其中一个怪癖是，他不能像 nvm 一样运行 bash 工具。 为此，你需要一个叫做 bass的包， 来将 nvm 暴露给 Fish Shell。\nbass 创建了一个可以支持其他 bash 工具包的框架， 比如我们稍后会使用一个叫 fish-nvm 的工具包。 也有很多其他用于 nvm 的工具包，但是 fish-nvm 不会影响性能。\n$ omf install bass $ omf install https://github.com/FabioAntunes/fish-nvm 使用 virtualenv 这是使用 Fish Shell 的一个陷阱。当你在 Python 的虚拟环境工作时， 你 不能 使用 下面这样普通的方式激活虚拟环境：\n$ python -m venv venv $ source venv/bin/activate 而是要使用下面的方式：\n$ source venv/bin/activate.fish 高效的包 pj (译者注：翻译的挺烂的，原文写的也不咋地，主要看图和gif大概就能懂)\npj 会以一种预测的方式，在你喜欢的目录之间跳转。 告诉 pj 去哪里寻找你的项目或者文件夹， 然后他可以通过 tab 来补全。\n$ omf install pj 比如，在 home 目录下有个 test 文件夹，里面有一堆别的文件夹。 为了将 test 文件夹标记为跳转目标，我们需要设置这样的项目地址：\n$ set -Ux PROJECT_PATHS ~/test 现在，我们可以在任意位置来访问 test 内的文件夹了。 pj 的作用\nz z 和 pj 有些相似，但从某种意义上来说 z 更加智能， 它会持续跟踪你最常访问的一些文件夹，因此你可以轻松的跳到这些位置。\n$ omf install z 就像我说的，z 是一个智能的工具，即使我输入了错别字， 它也会从我的最常访问里努力匹配到与输入最相近的那一个。 z 的作用\nplugin-git 和 zsh 中的 git 插件类似，plugin-git 包会给予你一个标准 git 别名集合 来加速你的 git 工作流。\n$ omf install https://github.com/jhillyerd/plugin-git plugin-git 的作用\n不只这样，为了确保你使用正确的别名， 它也会将别名进行展开，来形成完整的命令。 这里有完整的别名列表。\nfzf Fuzzy Finder(模糊查找) 或者 fzf 是一种更加快速的通用查找工具， 可以用它来查找文件或者命令历史。\n$ omf install https://github.com/jethrokuan/fzf 搜索遍历你的命令历史记录，你可以使用 ctrl + r 或者输入该命令的某些部分， 然后敲击 ctrl + r 来精准查找符合条件的命令。 fzf 的使用\n如果你想在当前目录下搜索文件，你可以使用 ctrl + o 然后浏览他们。 你可以用这个工具做更多事情，点击这里查看更多。\n总结 我希望这篇文章能很好的指导安装 Fish 和提升工作流的效率。如果你有任何建议或问题， 下面评论就好~\n","date":"2020-07-30T09:12:49+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShEod.jpg","permalink":"https://aiar.site/post/f4273dc3df9447f2ae8f9c58d9df142e/","title":"[译] Fish 命令行的生产效率"},{"content":" 原文地址：Git Concepts I Wish I Knew Years Ago 原文作者：Gabriel Abud 译文出自：DEV Community 译者：Arrackisarookie 我辈开发者使用最多的技术既不是 JavaScript， 也不是 Python 或者 HTML。 它甚至在面试中都很少被提到，也很少被列入工作的必备技术栈。\n没错，我说的正是 Git 和版本控制。\n长久以来，我们大部分开发人员只学过一点点 Git 的概念。 这些知识仅仅能让我们拥有在一个小团队内使用简单功能分支工作流的能力。 如果你也像我一样，这种状态将会伴随你的职业生涯很久。\n是时候再访 Git，重新审视一下掌握它对我们职业生涯的提升有多么重要。 本指南可以作为一篇参考，它包含了一些我认为很是重要但可能鲜为人知的概念。 掌握 Git 之后，你管理代码的方式以及每天的工作流将会发生巨大的改变。 由于 Git 命令有些陈旧并且难以记忆，因此本文将会按照概念和预期表现进行分解。\n如果你对 Git 的基本概念掌握的不够牢固，比如工作目录、本地仓库和远程仓库之间的区别， 那么建议你可以先阅读这篇指南。 同样，如果没有掌握 Git 的基本命令，可以从官方文档开始学习。 本文并不意味着会带你从一个彻底的新手变成专业人员， 而是默认你已经熟练掌握如何使用 Git。\n基本 Git 命令 日志 我刚干哈了 $ git log $ git log --oneline # 更为精炼的输出 $ git log --graph # 以分支的可视化图显示 查看你的撤销历史 $ git reflag 因为有时 git log 命令无法捕捉到撤销的命令， 特别是对于那些无法在 commit 历史里显示的命令。\n在你运行了类似 git rebase 这样的“危害型”命令后， reflag 基本上可以算是一层安全网。 你不仅可以看到之前所做的 commit， 而且还将看到导致 commit 的每一个过程。 看这篇 Atlassian 上的文章 来了解更多关于 refs 运作方式。\n查看当前状态 + 任何合并冲突 $ git status 虽然 git status 是一个非常基础的命令，我们很早之前就学过， 但是，由于它作为 Git 内部基本原理的学习工具，其重要程度仍然值得 我们重复学习。 它还可以帮助你浏览复杂的 rebase 和 merge 过程。\n对比 staged (或者 unstaged) 中的异同 $ git diff --staged # staged 的改变 $ git diff # unstaged 的改变 对比两个分支之间的异同 $ git diff branch1..branch2 导航 (Navigation) 我想看看我之前干哈了 $ git reset \u0026lt;commit-sha\u0026gt; 这条命令将会撤销对应 commit，并且取消那次 commit 中的 stage 操作， 但是那些文件仍然保留在工作目录。\n我想切换到别的分支 $ git switch branch-name # Git 2.23 中的新语法 $ git checkout branch-name # 经典语法 git checkout 可能会有些让人难以理解，因为他既可以工作在文件层级， 也可以工作在分支层级。 从 Git 2.23 开始，我们拥有了两个新的命令：\ngit restore 用来 checkout 文件 git switch 用来 checkout 分支 (译者注：详细可访问 官方文档 和 Stack Overflow 相关提问)\n如果你想避免 git checkout 造成的困扰，上面两个命令非常适合你。\n我想回到之前我在的分支 $ git switch - 修改 (Modifications) 我把自己挖进了兔子洞，让我们重新开始 (译者注: get 不到这个梗。。)\n$ git reset --hard HEAD 这条命令将重置本地目录到最近一次 commit 的状态，并且会放弃所有 unstage 的文件。\n我想把一个文件重置到之前的样子 $ git restore \u0026lt;filename\u0026gt; # Git 2.23 新语法 $ git checkout -- \u0026lt;filename\u0026gt; # 经典语法 我想撤销上一次 commit 并且重写历史 $ git reset --hard HEAD~1 我想回到 n 次 commit 之前 $ git reset --hard HEAD~n # 回到倒数第 n 次 commit $ git reset --hard \u0026lt;commit-sha\u0026gt; # 或者回到特定的某次提交 soft，mixed 和 hard 三种 reset 的不同：\n--soft：撤销 commit 但是工作目录中会保留更改 --mixed (默认)：撤销 commit，撤销当次 commit 的 stage，但是工作目录中会保留更改 --hard：撤销 commit，撤销当次 commit 的 stage，并且删除更改 我已经重写了历史记录，现在想把这些改变 push 到远程仓库 $ git push -f 只要你的本地仓库和远程仓库有差异，这一步都是必要的。\nWARNING：强制 push 需要格外小心。一般来说， 在共享的分支你应该避免任何的强制 push。在开启一个 pull 请求之前， 你应将强制 push 限制在你自己的分支内，以免在不经意间弄乱你队友的 git 历史。\n我想为上一次 commit 多加一些改变 $ git commit --amend 我想重写本地的一堆 commit $ git rebase -i \u0026lt;commit hash\u0026gt; # commit hash 是所有你想改变的 commit 之前的一个 commit 的 hash 这条命令将开启一个互动提示，你可以通过它来选择保持、压缩、删除 哪一个 commit。你也可以在这里改变 commit message。 比如在清理错字或者规范化 commit 时，它非常有用。\n当深入学习 Git 之后，我发现 rebasing 是非常令人困惑的主题之一。 查看这个 rebasing 文档了解更多。\n这个 rebase 垮了，报废掉它吧 $ git rebase --abort 你可以在 rebase 过程中使用这条命令。\n我发现，rebase 带来的麻烦总是超过他的价值，特别是在 rebase 两个 有着大量相同更改的分支的时候。在完成整个 rebase 之前， 你都可以让这个 rebase 流产。\n我想从另一个分支把一个 commit 带到当前分支 # 将 commit-sha 所指的 commit 带入当前分支 $ git cherry-pick \u0026lt;commit-sha\u0026gt; 我想从另一个分支把一个指定的文件带到当前分支 $ git checkout \u0026lt;branch-name\u0026gt; \u0026lt;filename\u0026gt; (译者注：这个仿佛不能用 git restore，git restore 更倾向于重置和恢复)\n我想在版本控制中停止追踪某个文件 $ git rm --cached \u0026lt;file-name\u0026gt; 我需要更换分支，但当前状态已有更改 $ git stash # 将已有更改保存在 stash 栈的栈顶 $ git stash save \u0026#34;对于更改的信息描述\u0026#34; $ git stash -u # 同时也 stash 未被追踪 (untracked) 的文件 我想看看我的 stash 里有啥 $ git stash list 我想把 stash 里的东西取出来 $ git stash pop # 弹出最近添加到 stash 栈的项目 $ git stash apply stash@{stash_index} # 申请取出指定项目可以用 git stash list 查看 我想撤销一次 commit 而不重写历史 $ git revert HEAD # 撤销最近一次 commit $ git revert \u0026lt;commit-sha\u0026gt; # 撤销指定 commit 这条命令将重新运行提交新的 commit 时的逆过程， 从而撤销你的更改而不会撤销历史记录。 在共享的分支中撤销 commit 时，重写历史记录会非常的复杂， 所以使用 git revert 是一种很安全的解决方式。\n清理 (Cleanup) 我去，咋有这么多分支 $ git branch --no-color --merged | command grep -vE \u0026#34;^(\\+|\\*|\\s*(master|develop|dev)\\s*$)\u0026#34; | command xargs -n 1 git branch -d 这条命令将删除本地除了 master、develop、dev 之外的所有已合并的分支， 如果你的主分支和 dev 分支有着另外的名字， 你可以改变相应的 grep 的正则。\n这条命令很长，不太好记，但你可以为它设置一个别名，就像这样：\n$ alias gbda=\u0026#39;git branch --no-color --merged | command grep -vE \u0026#34;^(\\+|\\*|\\s*(master|develop|dev)\\s*$)\u0026#34; | command xargs -n 1 git branch -d\u0026#39; 如果你在使用 Oh My Zsh ，这一步它已经为你完成。 查看 aliases 了解更多。\n来清理旧分支和无效 commit 吧~ $ git fetch --all --prune 如果你已经为远程仓库设置了在 merge 时删除分支，这条命令也非常有用。\n(译者注：git fetch \u0026ndash;prune 将会在 fetch 前 移除在本地的所有远程仓库中不再存在的远程跟踪引用，详见官方文档)\nAliases Git 命令有时会很长，不太容易记住。我们不想每次都把它们敲一遍或者 花费几天将它们背下来，因此我强烈建议你为它们设置 Git 别名。\n更方便的方式是，安装一个像 Z Shell (Zsh) 中的 Oh My Zsh 一样的工具。 这样一来，你将拥有一大堆最常用的 Git 命令的别名。 你可以使用 别名 + tab 来补全他们。我懒得按照我的喜好设置 shell， 所以我喜欢用一些类似 Oh My Zsh 的开源工具，它们可以帮我配置好~ 更不用说它们还有这漂亮的外观了~\n我每天用的最多的一些命令：\n$ gst - git status $ gc - git commit $ gaa - git add --all $ gco - git checkout $ gp - git push $ gl - git pull $ gcb - git checkout -b $ gm - git merge $ grb - git rebase $ gpsup - git push --set-upstream origin $(current_branch) $ gbda - git branch --no-color --merged | command grep -vE \u0026#34;^(\\+|\\*|\\s*(master|develop|dev)\\s*$)\u0026#34; | command xargs -n 1 git branch -d $ gfa - git fetch --all --prune 如果你忘记了这些或者其他你自己设置的别名，可以运行：\n$ alias 或者给出关键词进行搜索：\n$ alias grep \u0026lt;alias-name\u0026gt; 其他 Git 技巧 忽视文件 (Ignoring Files) 很多文件可能不需要存在于版本控制中，你可以设置全局 gitignore 文件 来忽视掉它们。需要忽视的文件可能是一些 node_modules 文件夹， .vscode 或其他 IDE 的文件，以及一些 Python 的虚拟环境。\n对于一些敏感信息，你可以使用环境变量文件存放，然后将它们添加到 项目根目录下的 .gitignore 文件中。\n特殊文件 (Special Files) 你可能需要将一些文件标记为二进制文件，以便于 Git 可以将其忽视， 并且不用为它们产生冗长的差异性检测。Git 有一个 .gitattributes 文件 来实现这一操作。比如在一个 JavaScript 项目中， 你会在 .gitattributes 中添加一个 yarn-lock.json 或者 package-lock.json，这样一来，在你每次更新时， Git 不用每次都尝试记录它们的差异变化。\n# .gitattributes package-lock.json binary Git 工作流 Rebase vs. Merge 你的团队可能会从 rebase 和 merge 两种工作流中二选其一， 二者都有利弊，我曾见过这两种方式都可以产生很高的效率。 对于大多数情况，除非你 真的 了解你正在做什么， 否则选择 merge 工作流就完事了。\n当你主要使用 merge 来为产品更新迭代时， 仍然也可以高效的使用 rebase。 最常见的场景是，你正在一个 feature 上工作， 同时另外一个开发者 pull 了一个别的 feature 到 master。 你确实可以使用 git merge 将那些改变一起带上， 但是这样，你会对队友做的简单更改有一个额外的 commit。 你真正想做的是将你的提交 重新提交 到最新的 master 的最上面。\n$ git rebase master 这条命令将给你一个更干净的 commit 历史。\n深度解释它们之间的不同点可能需要一整篇论文来阐述， 因此，我建议你可以查阅 the Atlassian docs 中有关这些差异的文章。\n远程仓库设置 (Remote Repository Setting) 我对于 GitHub 和 GitLab 最为熟悉，但是其他远程仓库管理器也 应该支持这些设置。\n1. 在 merge 时删除分支 一旦有分支被 merge，你就不应该在关心这个分支， 因为它的历史将被映射到你的 master/dev 分支。 这一举措会显著的减少你所管理的分支数量。 也可以通过使用 git fetch -all --prune 更为高效的保持本地仓库的干净整洁。\n2. 防止直接 push 到 master 如果没有这个设置，很容易在 git push 时，忘记自己正在 master， 这会潜在的破坏你的产品，一点也不好。\n3. merge 前至少需要一次确认 取决于团队的大小，你可能需要在 merge 前需要多次的确认， 即使只是一个二人团队，最少也要确认一次。 你不用花费几个小时每行都看，但一般来说， 你的代码应该至少有两个人看过。 反馈 是学习和个人提升的关键。\n4. merge 前需要通过 CI 测试 (译者注：CI 即 Continuous Integration，持续集成)\n已损坏的改变不应该被 merge 到产品中。 测试人员无法 100% 的捕捉损坏的更改，因此需要自动执行这些检测。\nPull 请求 (Pull Requests) 保持 Pull 请求小而简洁，理想情况下不超过几百行。 小而频繁的 Pull 请求会使得审阅过程更快，从而产生更多的无 bug 代码， 也会让你的队友更轻松，从而提升团队的效率，也更容易分享学习经验。 团队内部达成一个共同的承诺，承诺每天都花一些时间来审阅公开 Pull 请求。\n我们都爱这样的审阅： ![reviewing]https://res.cloudinary.com/practicaldev/image/fetch/s\u0026ndash;7vLB7w1a\u0026ndash;/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/gdcwrvn006gryk0xiox0.png)\n如果你正在实现的特性在一段时间内都会处于损坏状态， 请使用特性标签来在产品中禁用它。 这将会防止你的特性分支和 dev/master 分支产生太大差异， 同时也允许你做更频繁，更小的 Pull 请求。不 merge 代码的时间越长， 以后 merge 的难度就越大。\n最后，在你的 Pull 请求中放一个细节描述，如果必要的话， 可以放图片或者 GIF。如果你使用像 Jira 这样的工具管理票据 (tickets，译者注：没使过不太懂啥意思)， 描述中也可以包括 Pull 请求地址的票据的编号。 Pull 请求的描述和可视化做的越详细，可能你的队友就越想审阅你的代码， 而不是拖延着下次一定。\n分支命名 (Branch Naming) 你的团队可能会提出分支命名的规范，以便于导航。 我喜欢每个分支以创建人的名字首字母开头，接着一个左斜杠， 然后是以短横线连接的分支描述。\n这可能看起来微不足道，但是配合 tab 补全以及类似 grep 这样的工具一起使用， 这确实可以帮你找到并理解可能有的所有分支。\n例如，我创建了一个新分支：\n$ git checkout -b gabud/implement-important-feature 一周后，当我忘记我之前给它起了什么名字， 我可以键入 git checkout gabud，然后按 tab 键， 我的 Z shell 就会向我展示所有我本地的分支以供选择， 而不用看我队友们的分支。\n提交信息 (Commit Messages) 语言很重要，一般来说，我发现最好不要以破碎状态提交东西， 每一次提交都应该有一个简洁的信息，说明所做的更改。 按照官方 Git 建议，我发现最好使用当前的命令式意义来提交信息。 可以将每个提交信息视为对 计算机/git 的命令， 以至于你可以将提交信息加到这句话后面：\n如果这个 commit 被应用，将会\u0026hellip;\n在当前命令式意义上，良好的提交示例为：\n$ git commit -m \u0026#34;Add name field to checkout form\u0026#34; 现在可以这么读：“如果这个 commit 被应用，将会在表单中添加姓名域。”\n最后的想法 这绝不是已经了解了 Git 的全部，建议查阅 官方文档 和 git help 了解更多。 不要害怕向你的队友问一些 Git 的问题， 你会惊讶的发现大多数队友也有许多相同的问题。\n那么你呢？哪个 Git 命令或者概念在你的工作流中最有用呢~\n","date":"2020-07-03T08:54:48+08:00","image":"https://s11.ax1x.com/2024/01/08/pFShZFA.png","permalink":"https://aiar.site/post/cd5224ee1b194218bb7c286582bb4db2/","title":"[译] 相见恨晚的 Git 命令"}]